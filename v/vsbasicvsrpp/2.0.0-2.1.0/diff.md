# Comparing `tmp/vsbasicvsrpp-2.0.0-py3-none-any.whl.zip` & `tmp/vsbasicvsrpp-2.1.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,28 +1,27 @@
-Zip file size: 28444 bytes, number of entries: 26
--rw-r--r--  2.0 fat    11364 b- defN 20-Feb-02 00:00 vsbasicvsrpp/__init__.py
+Zip file size: 27527 bytes, number of entries: 25
+-rw-r--r--  2.0 fat    11273 b- defN 20-Feb-02 00:00 vsbasicvsrpp/__init__.py
 -rw-r--r--  2.0 fat     1563 b- defN 20-Feb-02 00:00 vsbasicvsrpp/__main__.py
--rw-r--r--  2.0 fat      952 b- defN 20-Feb-02 00:00 vsbasicvsrpp/basicvsr.py
--rw-r--r--  2.0 fat     7806 b- defN 20-Feb-02 00:00 vsbasicvsrpp/basicvsr_net.py
--rw-r--r--  2.0 fat    17062 b- defN 20-Feb-02 00:00 vsbasicvsrpp/basicvsr_pp.py
--rw-r--r--  2.0 fat     1335 b- defN 20-Feb-02 00:00 vsbasicvsrpp/builder.py
--rw-r--r--  2.0 fat     1935 b- defN 20-Feb-02 00:00 vsbasicvsrpp/flow_warp.py
--rw-r--r--  2.0 fat      995 b- defN 20-Feb-02 00:00 vsbasicvsrpp/logger.py
--rw-r--r--  2.0 fat      234 b- defN 20-Feb-02 00:00 vsbasicvsrpp/registry.py
--rw-r--r--  2.0 fat     3075 b- defN 20-Feb-02 00:00 vsbasicvsrpp/sr_backbone_utils.py
--rw-r--r--  2.0 fat     1557 b- defN 20-Feb-02 00:00 vsbasicvsrpp/upsample.py
+-rw-r--r--  2.0 fat      864 b- defN 20-Feb-02 00:00 vsbasicvsrpp/basicvsr.py
+-rw-r--r--  2.0 fat     7873 b- defN 20-Feb-02 00:00 vsbasicvsrpp/basicvsr_net.py
+-rw-r--r--  2.0 fat    16157 b- defN 20-Feb-02 00:00 vsbasicvsrpp/basicvsr_plusplus_net.py
+-rw-r--r--  2.0 fat     2392 b- defN 20-Feb-02 00:00 vsbasicvsrpp/flow_warp.py
+-rw-r--r--  2.0 fat     1287 b- defN 20-Feb-02 00:00 vsbasicvsrpp/model_utils.py
+-rw-r--r--  2.0 fat      532 b- defN 20-Feb-02 00:00 vsbasicvsrpp/registry.py
+-rw-r--r--  2.0 fat     1952 b- defN 20-Feb-02 00:00 vsbasicvsrpp/sr_backbone.py
+-rw-r--r--  2.0 fat     1623 b- defN 20-Feb-02 00:00 vsbasicvsrpp/upsample.py
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/basicvsr_plusplus_c128n25_ntire_decompress_track1_20210223-7b2eba02.pth
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/basicvsr_plusplus_c128n25_ntire_decompress_track2_20210314-eeae05e6.pth
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/basicvsr_plusplus_c128n25_ntire_decompress_track3_20210304-6daf4a40.pth
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/basicvsr_plusplus_c128n25_ntire_vsr_20210311-1ff35292.pth
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/basicvsr_plusplus_c64n7_8x1_300k_vimeo90k_bd_20210305-ab315ab1.pth
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/basicvsr_plusplus_c64n7_8x1_300k_vimeo90k_bi_20210305-4ef437e2.pth
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/basicvsr_plusplus_c64n7_8x1_600k_reds4_20210217-db622b2f.pth
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/basicvsr_plusplus_deblur_dvd-ecd08b7f.pth
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/basicvsr_plusplus_deblur_gopro-3c5bb9b5.pth
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/basicvsr_plusplus_denoise-28f6920c.pth
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsbasicvsrpp/models/spynet_20210409-c6c1bd09.pth
-?rw-r--r--  2.0 fat    14389 b- defN 20-Feb-02 00:00 vsbasicvsrpp-2.0.0.dist-info/METADATA
-?rw-r--r--  2.0 fat       87 b- defN 20-Feb-02 00:00 vsbasicvsrpp-2.0.0.dist-info/WHEEL
-?rw-r--r--  2.0 fat    11558 b- defN 20-Feb-02 00:00 vsbasicvsrpp-2.0.0.dist-info/licenses/LICENSE
-?rw-r--r--  2.0 fat     2655 b- defN 20-Feb-02 00:00 vsbasicvsrpp-2.0.0.dist-info/RECORD
-26 files, 76567 bytes uncompressed, 23924 bytes compressed:  68.8%
+?rw-r--r--  2.0 fat    14422 b- defN 20-Feb-02 00:00 vsbasicvsrpp-2.1.0.dist-info/METADATA
+?rw-r--r--  2.0 fat       87 b- defN 20-Feb-02 00:00 vsbasicvsrpp-2.1.0.dist-info/WHEEL
+?rw-r--r--  2.0 fat    11558 b- defN 20-Feb-02 00:00 vsbasicvsrpp-2.1.0.dist-info/licenses/LICENSE
+?rw-r--r--  2.0 fat     2585 b- defN 20-Feb-02 00:00 vsbasicvsrpp-2.1.0.dist-info/RECORD
+25 files, 74168 bytes uncompressed, 23111 bytes compressed:  68.8%
```

## zipnote {}

```diff
@@ -6,30 +6,27 @@
 
 Filename: vsbasicvsrpp/basicvsr.py
 Comment: 
 
 Filename: vsbasicvsrpp/basicvsr_net.py
 Comment: 
 
-Filename: vsbasicvsrpp/basicvsr_pp.py
-Comment: 
-
-Filename: vsbasicvsrpp/builder.py
+Filename: vsbasicvsrpp/basicvsr_plusplus_net.py
 Comment: 
 
 Filename: vsbasicvsrpp/flow_warp.py
 Comment: 
 
-Filename: vsbasicvsrpp/logger.py
+Filename: vsbasicvsrpp/model_utils.py
 Comment: 
 
 Filename: vsbasicvsrpp/registry.py
 Comment: 
 
-Filename: vsbasicvsrpp/sr_backbone_utils.py
+Filename: vsbasicvsrpp/sr_backbone.py
 Comment: 
 
 Filename: vsbasicvsrpp/upsample.py
 Comment: 
 
 Filename: vsbasicvsrpp/models/basicvsr_plusplus_c128n25_ntire_decompress_track1_20210223-7b2eba02.pth
 Comment: 
@@ -60,20 +57,20 @@
 
 Filename: vsbasicvsrpp/models/basicvsr_plusplus_denoise-28f6920c.pth
 Comment: 
 
 Filename: vsbasicvsrpp/models/spynet_20210409-c6c1bd09.pth
 Comment: 
 
-Filename: vsbasicvsrpp-2.0.0.dist-info/METADATA
+Filename: vsbasicvsrpp-2.1.0.dist-info/METADATA
 Comment: 
 
-Filename: vsbasicvsrpp-2.0.0.dist-info/WHEEL
+Filename: vsbasicvsrpp-2.1.0.dist-info/WHEEL
 Comment: 
 
-Filename: vsbasicvsrpp-2.0.0.dist-info/licenses/LICENSE
+Filename: vsbasicvsrpp-2.1.0.dist-info/licenses/LICENSE
 Comment: 
 
-Filename: vsbasicvsrpp-2.0.0.dist-info/RECORD
+Filename: vsbasicvsrpp-2.1.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## vsbasicvsrpp/__init__.py

```diff
@@ -1,25 +1,23 @@
 from __future__ import annotations
 
-import logging
 import math
 import os
 
 import numpy as np
 import torch
 import torch.nn.functional as F
 import vapoursynth as vs
-from mmcv.runner import load_checkpoint
+from mmengine.runner import load_checkpoint
 
 from .basicvsr import BasicVSR
-from .basicvsr_pp import BasicVSRPlusPlus
-from .builder import build_model
-from .logger import get_root_logger
+from .basicvsr_plusplus_net import BasicVSRPlusPlusNet
+from .registry import MODELS
 
-__version__ = "2.0.0"
+__version__ = "2.1.0"
 
 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
 
 model_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "models")
 
 
 @torch.inference_mode()
@@ -145,26 +143,25 @@
 
     model_path = os.path.join(model_dir, model_name)
     spynet_path = os.path.join(model_dir, "spynet_20210409-c6c1bd09.pth")
 
     cfg = dict(
         type="BasicVSR",
         generator=dict(
-            type="BasicVSRPlusPlus",
+            type="BasicVSRPlusPlusNet",
             mid_channels=mid_channels,
             num_blocks=num_blocks,
             is_low_res_input=is_low_res_input,
             spynet_pretrained=spynet_path,
             cpu_cache=cpu_cache,
         ),
     )
 
-    module = build_model(cfg)
-    logger = get_root_logger(log_level=logging.WARNING)
-    load_checkpoint(module, model_path, map_location="cpu", logger=logger)
+    module = MODELS.build(cfg)
+    load_checkpoint(module, model_path, map_location="cpu", logger="silent")
     module.eval().to(device)
     if fp16:
         module.half()
 
     pad_w = math.ceil(max(clip.width, min_res) / modulo) * modulo
     pad_h = math.ceil(max(clip.height, min_res) / modulo) * modulo
```

## vsbasicvsrpp/basicvsr.py

```diff
@@ -1,12 +1,11 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import torch
 import torch.nn as nn
 
-from .builder import build_backbone
 from .registry import MODELS
 
 
 @MODELS.register_module()
 class BasicVSR(nn.Module):
     """BasicVSR model for video super-resolution.
 
@@ -20,19 +19,16 @@
         generator (dict): Config for the generator structure.
     """
 
     def __init__(self, generator):
         super().__init__()
 
         # generator
-        self.generator = build_backbone(generator)
+        self.generator = MODELS.build(generator)
 
         # count training steps
         self.register_buffer('step_counter', torch.zeros(1))
 
-    def forward(self, lq):
-        """Forward function.
+    def forward(self, inputs):
+        """Forward tensor. Returns result of simple forward."""
 
-        Args:
-            lq (Tensor): LQ Tensor with shape (n, t, c, h, w).
-        """
-        return self.generator(lq)
+        return self.generator(inputs)
```

## vsbasicvsrpp/basicvsr_net.py

```diff
@@ -1,22 +1,22 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-import logging
-
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.cnn import ConvModule
-from mmcv.runner import load_checkpoint
+from mmengine import MMLogger
+from mmengine.model import BaseModule
+from mmengine.runner import load_checkpoint
 
 from .flow_warp import flow_warp
-from .logger import get_root_logger
-from .sr_backbone_utils import ResidualBlockNoBN, make_layer
+from .model_utils import make_layer
+from .sr_backbone import ResidualBlockNoBN
 
 
-class ResidualBlocksWithInputConv(nn.Module):
+class ResidualBlocksWithInputConv(BaseModule):
     """Residual blocks with a convolution in front.
 
     Args:
         in_channels (int): Number of input channels of the first conv.
         out_channels (int): Number of channels of the residual blocks.
             Default: 64.
         num_blocks (int): Number of residual blocks. Default: 30.
@@ -46,15 +46,15 @@
 
         Returns:
             Tensor: Output feature with shape (n, out_channels, h, w)
         """
         return self.main(feat)
 
 
-class SPyNet(nn.Module):
+class SPyNet(BaseModule):
     """SPyNet network structure.
 
     The difference to the SPyNet in [tof.py] is that
         1. more SPyNetBasicModule is used in this version, and
         2. no batch normalization is used in this version.
 
     Paper:
@@ -67,16 +67,15 @@
     def __init__(self, pretrained):
         super().__init__()
 
         self.basic_module = nn.ModuleList(
             [SPyNetBasicModule() for _ in range(6)])
 
         if isinstance(pretrained, str):
-            logger = get_root_logger(log_level=logging.WARNING)
-            load_checkpoint(self, pretrained, strict=True, logger=logger)
+            load_checkpoint(self, pretrained, strict=True, logger="silent")
         elif pretrained is not None:
             raise TypeError('[pretrained] should be str or None, '
                             f'but got {type(pretrained)}.')
 
         self.register_buffer(
             'mean',
             torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
@@ -158,34 +157,36 @@
         """
 
         # upsize to a multiple of 32
         h, w = ref.shape[2:4]
         w_up = w if (w % 32) == 0 else 32 * (w // 32 + 1)
         h_up = h if (h % 32) == 0 else 32 * (h // 32 + 1)
         ref = F.interpolate(
-            input=ref, size=(h_up, w_up), mode='bilinear')
+            input=ref, size=(h_up, w_up), mode='bilinear', align_corners=False)
         supp = F.interpolate(
             input=supp,
             size=(h_up, w_up),
-            mode='bilinear')
+            mode='bilinear',
+            align_corners=False)
 
         # compute flow, and resize back to the original resolution
         flow = F.interpolate(
             input=self.compute_flow(ref, supp),
             size=(h, w),
-            mode='bilinear')
+            mode='bilinear',
+            align_corners=False)
 
         # adjust the flow values
         flow[:, 0, :, :] *= float(w) / float(w_up)
         flow[:, 1, :, :] *= float(h) / float(h_up)
 
         return flow
 
 
-class SPyNetBasicModule(nn.Module):
+class SPyNetBasicModule(BaseModule):
     """Basic Module for SPyNet.
 
     Paper:
         Optical Flow Estimation using a Spatial Pyramid Network, CVPR, 2017
     """
 
     def __init__(self):
```

## vsbasicvsrpp/flow_warp.py

```diff
@@ -26,25 +26,34 @@
     """
     if x.size()[-2:] != flow.size()[1:3]:
         raise ValueError(f'The spatial sizes of input ({x.size()[-2:]}) and '
                          f'flow ({flow.size()[1:3]}) are not the same.')
     _, _, h, w = x.size()
     # create mesh grid
     device = flow.device
-    grid_y, grid_x = torch.meshgrid(
-        torch.arange(0, h, device=device, dtype=x.dtype),
-        torch.arange(0, w, device=device, dtype=x.dtype), indexing='ij')
+    # torch.meshgrid has been modified in 1.10.0 (compatibility with previous
+    # versions), and will be further modified in 1.12 (Breaking Change)
+    if 'indexing' in torch.meshgrid.__code__.co_varnames:
+        grid_y, grid_x = torch.meshgrid(
+            torch.arange(0, h, device=device, dtype=x.dtype),
+            torch.arange(0, w, device=device, dtype=x.dtype),
+            indexing='ij')
+    else:
+        grid_y, grid_x = torch.meshgrid(
+            torch.arange(0, h, device=device, dtype=x.dtype),
+            torch.arange(0, w, device=device, dtype=x.dtype))
     grid = torch.stack((grid_x, grid_y), 2)  # h, w, 2
     grid.requires_grad = False
 
     grid_flow = grid + flow
     # scale grid_flow to [-1,1]
     grid_flow_x = 2.0 * grid_flow[:, :, :, 0] / max(w - 1, 1) - 1.0
     grid_flow_y = 2.0 * grid_flow[:, :, :, 1] / max(h - 1, 1) - 1.0
     grid_flow = torch.stack((grid_flow_x, grid_flow_y), dim=3)
+    grid_flow = grid_flow.type(x.type())
     output = F.grid_sample(
         x,
         grid_flow,
         mode=interpolation,
         padding_mode=padding_mode,
         align_corners=align_corners)
     return output
```

## vsbasicvsrpp/registry.py

```diff
@@ -1,8 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from mmcv.cnn import MODELS as MMCV_MODELS
-from mmcv.utils import Registry
+"""Registries and utilities in MMagic.
 
-MODELS = Registry('model', parent=MMCV_MODELS)
-BACKBONES = MODELS
-COMPONENTS = MODELS
-LOSSES = MODELS
+MMagic provides 17 registry nodes to support using modules across projects.
+Each node is a child of the root registry in MMEngine.
+
+More details can be found at
+https://mmengine.readthedocs.io/en/latest/tutorials/registry.html.
+"""
+
+from mmengine.registry import MODELS as MMENGINE_MODELS
+from mmengine.registry import Registry
+
+# Neural network modules inheriting `nn.Module`.
+MODELS = Registry('model', parent=MMENGINE_MODELS)
```

## vsbasicvsrpp/upsample.py

```diff
@@ -1,12 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
 import torch.nn.functional as F
+from torch import Tensor
 
-from .sr_backbone_utils import default_init_weights
+from .model_utils import default_init_weights
 
 
 class PixelShufflePack(nn.Module):
     """Pixel Shuffle upsample layer.
 
     Args:
         in_channels (int): Number of input channels.
@@ -14,33 +15,33 @@
         scale_factor (int): Upsample ratio.
         upsample_kernel (int): Kernel size of Conv layer to expand channels.
 
     Returns:
         Upsampled feature map.
     """
 
-    def __init__(self, in_channels, out_channels, scale_factor,
-                 upsample_kernel):
+    def __init__(self, in_channels: int, out_channels: int, scale_factor: int,
+                 upsample_kernel: int):
         super().__init__()
         self.in_channels = in_channels
         self.out_channels = out_channels
         self.scale_factor = scale_factor
         self.upsample_kernel = upsample_kernel
         self.upsample_conv = nn.Conv2d(
             self.in_channels,
             self.out_channels * scale_factor * scale_factor,
             self.upsample_kernel,
             padding=(self.upsample_kernel - 1) // 2)
         self.init_weights()
 
-    def init_weights(self):
+    def init_weights(self) -> None:
         """Initialize weights for PixelShufflePack."""
         default_init_weights(self, 1)
 
-    def forward(self, x):
+    def forward(self, x: Tensor) -> Tensor:
         """Forward function for PixelShufflePack.
 
         Args:
             x (Tensor): Input tensor with shape (n, c, h, w).
 
         Returns:
             Tensor: Forward results.
```

## Comparing `vsbasicvsrpp/basicvsr_pp.py` & `vsbasicvsrpp/basicvsr_plusplus_net.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,23 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-import logging
-
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-from mmcv.cnn import constant_init
 from mmcv.ops import ModulatedDeformConv2d, modulated_deform_conv2d
-from mmcv.runner import load_checkpoint
+from mmengine.model import BaseModule
+from mmengine.model.weight_init import constant_init
 
 from .basicvsr_net import ResidualBlocksWithInputConv, SPyNet
 from .flow_warp import flow_warp
-from .logger import get_root_logger
-from .registry import BACKBONES
+from .registry import MODELS
 from .upsample import PixelShufflePack
 
 
-@BACKBONES.register_module()
-class BasicVSRPlusPlus(nn.Module):
+@MODELS.register_module()
+class BasicVSRPlusPlusNet(BaseModule):
     """BasicVSR++ network structure.
 
     Support either x4 upsampling or same size output.
 
     Paper:
         BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation
         and Alignment
@@ -33,19 +30,16 @@
         max_residue_magnitude (int): The maximum magnitude of the offset
             residue (Eq. 6 in paper). Default: 10.
         is_low_res_input (bool, optional): Whether the input is low-resolution
             or not. If False, the output resolution is equal to the input
             resolution. Default: True.
         spynet_pretrained (str, optional): Pre-trained model path of SPyNet.
             Default: None.
-        cpu_cache (bool, optional): When the length of sequence is larger
-            than this value, the intermediate features are sent to CPU. This
-            saves GPU memory, but slows down the inference speed. You can
-            increase this number if you have a GPU with large memory.
-            Default: 100.
+        cpu_cache (bool, optional): Whether to cache the features in CPU.
+            Default: False.
     """
 
     def __init__(self,
                  mid_channels=64,
                  num_blocks=7,
                  max_residue_magnitude=10,
                  is_low_res_input=True,
@@ -97,28 +91,27 @@
         self.conv_last = nn.Conv2d(64, 3, 3, 1, 1)
         self.img_upsample = nn.Upsample(
             scale_factor=4, mode='bilinear', align_corners=False)
 
         # activation function
         self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)
 
-        # check if the sequence is augmented by flipping
-        self.is_mirror_extended = False
-
     def check_if_mirror_extended(self, lqs):
         """Check whether the input is a mirror-extended sequence.
 
         If mirror-extended, the i-th (i=0, ..., t-1) frame is equal to the
         (t-1-i)-th frame.
 
         Args:
             lqs (tensor): Input low quality (LQ) sequence with
                 shape (n, t, c, h, w).
         """
 
+        # check if the sequence is augmented by flipping
+        self.is_mirror_extended = False
         if lqs.size(1) % 2 == 0:
             lqs_1, lqs_2 = torch.chunk(lqs, 2, dim=1)
             if torch.norm(lqs_1 - lqs_2.flip(1)) == 0:
                 self.is_mirror_extended = True
 
     def compute_flow(self, lqs):
         """Compute optical flow using SPyNet for feature alignment.
@@ -167,52 +160,54 @@
         Return:
             dict(list[tensor]): A dictionary containing all the propagated
                 features. Each key in the dictionary corresponds to a
                 propagation branch, which is represented by a list of tensors.
         """
 
         n, t, _, h, w = flows.size()
-        device = flows.device
 
-        frame_idx = range(0, t + 1)
-        flow_idx = range(-1, t)
+        # PyTorch 2.0 could not compile data type of 'range'
+        # frame_idx = range(0, t + 1)
+        # flow_idx = range(-1, t)
+        frame_idx = list(range(0, t + 1))
+        flow_idx = list(range(-1, t))
         mapping_idx = list(range(0, len(feats['spatial'])))
         mapping_idx += mapping_idx[::-1]
 
         if 'backward' in module_name:
             frame_idx = frame_idx[::-1]
             flow_idx = frame_idx
 
         feat_prop = flows.new_zeros(n, self.mid_channels, h, w)
         for i, idx in enumerate(frame_idx):
             feat_current = feats['spatial'][mapping_idx[idx]]
             if self.cpu_cache:
-                feat_current = feat_current.cuda(device)
-                feat_prop = feat_prop.cuda(device)
+                feat_current = feat_current.cuda()
+                feat_prop = feat_prop.cuda()
             # second-order deformable alignment
             if i > 0:
                 flow_n1 = flows[:, flow_idx[i], :, :, :]
                 if self.cpu_cache:
-                    flow_n1 = flow_n1.cuda(device)
+                    flow_n1 = flow_n1.cuda()
 
                 cond_n1 = flow_warp(feat_prop, flow_n1.permute(0, 2, 3, 1))
 
                 # initialize second-order features
                 feat_n2 = torch.zeros_like(feat_prop)
                 flow_n2 = torch.zeros_like(flow_n1)
                 cond_n2 = torch.zeros_like(cond_n1)
 
                 if i > 1:  # second-order features
                     feat_n2 = feats[module_name][-2]
                     if self.cpu_cache:
-                        feat_n2 = feat_n2.cuda(device)
+                        feat_n2 = feat_n2.cuda()
 
                     flow_n2 = flows[:, flow_idx[i - 1], :, :, :]
                     if self.cpu_cache:
-                        flow_n2 = flow_n2.cuda(device)
+                        flow_n2 = flow_n2.cuda()
 
                     flow_n2 = flow_n1 + flow_warp(flow_n2,
                                                   flow_n1.permute(0, 2, 3, 1))
                     cond_n2 = flow_warp(feat_n2, flow_n2.permute(0, 2, 3, 1))
 
                 # flow-guided deformable convolution
                 cond = torch.cat([cond_n1, feat_current, cond_n2], dim=1)
@@ -222,15 +217,15 @@
 
             # concatenate and residual blocks
             feat = [feat_current] + [
                 feats[k][idx]
                 for k in feats if k not in ['spatial', module_name]
             ] + [feat_prop]
             if self.cpu_cache:
-                feat = [f.cuda(device) for f in feat]
+                feat = [f.cuda() for f in feat]
 
             feat = torch.cat(feat, dim=1)
             feat_prop = feat_prop + self.backbone[module_name](feat)
             feats[module_name].append(feat_prop)
 
             if self.cpu_cache:
                 feats[module_name][-1] = feats[module_name][-1].cpu()
@@ -249,28 +244,26 @@
                 shape (n, t, c, h, w).
             feats (dict): The features from the propagation branches.
 
         Returns:
             Tensor: Output HR sequence with shape (n, t, c, 4h, 4w).
         """
 
-        device = lqs.device
-
         outputs = []
         num_outputs = len(feats['spatial'])
 
         mapping_idx = list(range(0, num_outputs))
         mapping_idx += mapping_idx[::-1]
 
         for i in range(0, lqs.size(1)):
             hr = [feats[k].pop(0) for k in feats if k != 'spatial']
             hr.insert(0, feats['spatial'][mapping_idx[i]])
             hr = torch.cat(hr, dim=1)
             if self.cpu_cache:
-                hr = hr.cuda(device)
+                hr = hr.cuda()
 
             hr = self.reconstruction(hr)
             hr = self.lrelu(self.upsample1(hr))
             hr = self.lrelu(self.upsample2(hr))
             hr = self.lrelu(self.conv_hr(hr))
             hr = self.conv_last(hr)
             if self.is_low_res_input:
@@ -346,30 +339,14 @@
                 feats = self.propagate(feats, flows, module)
                 if self.cpu_cache:
                     del flows
                     torch.cuda.empty_cache()
 
         return self.upsample(lqs, feats)
 
-    def init_weights(self, pretrained=None, strict=True):
-        """Init weights for models.
-
-        Args:
-            pretrained (str, optional): Path for pretrained weights. If given
-                None, pretrained weights will not be loaded. Default: None.
-            strict (bool, optional): Whether strictly load the pretrained
-                model. Default: True.
-        """
-        if isinstance(pretrained, str):
-            logger = get_root_logger(log_level=logging.WARNING)
-            load_checkpoint(self, pretrained, strict=strict, logger=logger)
-        elif pretrained is not None:
-            raise TypeError(f'"pretrained" must be a str or None. '
-                            f'But received {type(pretrained)}.')
-
 
 class SecondOrderDeformableAlignment(ModulatedDeformConv2d):
     """Second-order deformable alignment module.
 
     Args:
         in_channels (int): Same as nn.Conv2d.
         out_channels (int): Same as nn.Conv2d.
@@ -399,17 +376,19 @@
             nn.LeakyReLU(negative_slope=0.1, inplace=True),
             nn.Conv2d(self.out_channels, 27 * self.deform_groups, 3, 1, 1),
         )
 
         self.init_offset()
 
     def init_offset(self):
+        """Init constant offset."""
         constant_init(self.conv_offset[-1], val=0, bias=0)
 
     def forward(self, x, extra_feat, flow_1, flow_2):
+        """Forward function."""
         extra_feat = torch.cat([extra_feat, flow_1, flow_2], dim=1)
         out = self.conv_offset(extra_feat)
         o1, o2, mask = torch.chunk(out, 3, dim=1)
 
         # offset
         offset = self.max_residue_magnitude * torch.tanh(
             torch.cat((o1, o2), dim=1))
```

## Comparing `vsbasicvsrpp/sr_backbone_utils.py` & `vsbasicvsrpp/sr_backbone.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,46 +1,12 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import torch.nn as nn
-from mmcv.cnn import constant_init, kaiming_init
-from mmcv.utils.parrots_wrapper import _BatchNorm
+from torch import Tensor
 
-
-def default_init_weights(module, scale=1):
-    """Initialize network weights.
-
-    Args:
-        modules (nn.Module): Modules to be initialized.
-        scale (float): Scale initialized weights, especially for residual
-            blocks.
-    """
-    for m in module.modules():
-        if isinstance(m, nn.Conv2d):
-            kaiming_init(m, a=0, mode='fan_in', bias=0)
-            m.weight.data *= scale
-        elif isinstance(m, nn.Linear):
-            kaiming_init(m, a=0, mode='fan_in', bias=0)
-            m.weight.data *= scale
-        elif isinstance(m, _BatchNorm):
-            constant_init(m.weight, val=1, bias=0)
-
-
-def make_layer(block, num_blocks, **kwarg):
-    """Make layers by stacking the same blocks.
-
-    Args:
-        block (nn.module): nn.module class for basic block.
-        num_blocks (int): number of blocks.
-
-    Returns:
-        nn.Sequential: Stacked blocks in nn.Sequential.
-    """
-    layers = []
-    for _ in range(num_blocks):
-        layers.append(block(**kwarg))
-    return nn.Sequential(*layers)
+from .model_utils import default_init_weights
 
 
 class ResidualBlockNoBN(nn.Module):
     """Residual block without BN.
 
     It has a style of:
 
@@ -52,42 +18,40 @@
     Args:
         mid_channels (int): Channel number of intermediate features.
             Default: 64.
         res_scale (float): Used to scale the residual before addition.
             Default: 1.0.
     """
 
-    def __init__(self, mid_channels=64, res_scale=1.0, groups=1):
+    def __init__(self, mid_channels: int = 64, res_scale: float = 1.0):
         super().__init__()
         self.res_scale = res_scale
-        self.conv1 = nn.Conv2d(
-            mid_channels, mid_channels, 3, 1, 1, bias=True, groups=groups)
-        self.conv2 = nn.Conv2d(
-            mid_channels, mid_channels, 3, 1, 1, bias=True, groups=groups)
+        self.conv1 = nn.Conv2d(mid_channels, mid_channels, 3, 1, 1, bias=True)
+        self.conv2 = nn.Conv2d(mid_channels, mid_channels, 3, 1, 1, bias=True)
 
         self.relu = nn.ReLU(inplace=True)
 
         # if res_scale < 1.0, use the default initialization, as in EDSR.
         # if res_scale = 1.0, use scaled kaiming_init, as in MSRResNet.
         if res_scale == 1.0:
             self.init_weights()
 
-    def init_weights(self):
+    def init_weights(self) -> None:
         """Initialize weights for ResidualBlockNoBN.
 
         Initialization methods like `kaiming_init` are for VGG-style modules.
         For modules with residual paths, using smaller std is better for
         stability and performance. We empirically use 0.1. See more details in
         "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks"
         """
 
         for m in [self.conv1, self.conv2]:
             default_init_weights(m, 0.1)
 
-    def forward(self, x):
+    def forward(self, x: Tensor) -> Tensor:
         """Forward function.
 
         Args:
             x (Tensor): Input tensor with shape (n, c, h, w).
 
         Returns:
             Tensor: Forward results.
```

## Comparing `vsbasicvsrpp-2.0.0.dist-info/METADATA` & `vsbasicvsrpp-2.1.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: vsbasicvsrpp
-Version: 2.0.0
+Version: 2.1.0
 Summary: BasicVSR++ function for VapourSynth
 Project-URL: Homepage, https://github.com/HolyWu/vs-basicvsrpp
 Project-URL: Bug Tracker, https://github.com/HolyWu/vs-basicvsrpp/issues
 Author-email: HolyWu <holywu@gmail.com>
 License:                                  Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
@@ -206,41 +206,43 @@
            WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
            See the License for the specific language governing permissions and
            limitations under the License.
 License-File: LICENSE
 Keywords: BasicVSR++,VapourSynth
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Topic :: Multimedia :: Video
 Requires-Python: >=3.10
-Requires-Dist: mmcv-full>=1.7.1
-Requires-Dist: numpy
-Requires-Dist: requests
-Requires-Dist: torch>=1.13.1
-Requires-Dist: torchvision>=0.14.1
-Requires-Dist: tqdm
-Requires-Dist: vapoursynth>=55
+Requires-Dist: mmcv>=2.0.0
+Requires-Dist: numpy>=1.24.3
+Requires-Dist: requests>=2.30.0
+Requires-Dist: torch>=2.0.1
+Requires-Dist: torchvision>=0.15.2
+Requires-Dist: tqdm>=4.65.0
+Requires-Dist: vapoursynth>=60
 Description-Content-Type: text/markdown
 
 # BasicVSR++
 Improving Video Super-Resolution with Enhanced Propagation and Alignment, based on https://github.com/ckkelvinchan/BasicVSR_PlusPlus.
 
 
 ## Dependencies
-- [mmcv-full](https://github.com/open-mmlab/mmcv#installation) 1.7.1
+- [mmcv](https://github.com/open-mmlab/mmcv#installation) >=2.0.0
 - [NumPy](https://numpy.org/install)
-- [PyTorch](https://pytorch.org/get-started) 1.13.1
-- [VapourSynth](http://www.vapoursynth.com/) R55+
+- [PyTorch](https://pytorch.org/get-started) >=2.0.1
+- [VapourSynth](http://www.vapoursynth.com/) >=R60
 
 
 ## Installation
 ```
-pip install -U openmim vsbasicvsrpp
-mim install "mmcv-full>=1.7.1"
+pip install -U openmim
+mim install "mmcv>=2.0.0"
+
+pip install -U vsbasicvsrpp
 python -m vsbasicvsrpp
 ```
 
 
 ## Usage
 ```python
 from vsbasicvsrpp import basicvsrpp
```

## Comparing `vsbasicvsrpp-2.0.0.dist-info/licenses/LICENSE` & `vsbasicvsrpp-2.1.0.dist-info/licenses/LICENSE`

 * *Files identical despite different names*

## Comparing `vsbasicvsrpp-2.0.0.dist-info/RECORD` & `vsbasicvsrpp-2.1.0.dist-info/RECORD`

 * *Files 18% similar despite different names*

```diff
@@ -1,26 +1,25 @@
-vsbasicvsrpp/__init__.py,sha256=UApYsLIBTWPQwklht3mJUmIMTuAPL0gPDIhgxBQ-pw4,11364
+vsbasicvsrpp/__init__.py,sha256=FwjkTLNTK5OSb1hQEpn4luJJNzkna0LkX4r4hTZEGhw,11273
 vsbasicvsrpp/__main__.py,sha256=JDNlvnmydlFvmiXGT1fV-HVZY1smrZtv9sSN6L4Kdk8,1563
-vsbasicvsrpp/basicvsr.py,sha256=6tiR9BWDLTOzj6d4zkti1KIBCU79svKn26gzubPVGeo,952
-vsbasicvsrpp/basicvsr_net.py,sha256=DLkJMhFwjk2dOpxem5OJGBBMMruCIrRQqyOqo163TM0,7806
-vsbasicvsrpp/basicvsr_pp.py,sha256=VphsIfHatNeU-Ihka6dwm8l04VoOh2pi48oI-HH8PVE,17062
-vsbasicvsrpp/builder.py,sha256=7M-KROFTjuP7plcWgCdbK7QLsgNSa1Z0E3Px34eEYnE,1335
-vsbasicvsrpp/flow_warp.py,sha256=2K9kDNaS_3ECB5eNMIHNcCqXS_5eg9SRqAqr-kVmqOc,1935
-vsbasicvsrpp/logger.py,sha256=i0LerILXn73O8RonlEL7Ez4qLK7qb_vxn7K5SaGt2Ak,995
-vsbasicvsrpp/registry.py,sha256=t-8UeXCB8aW2P03_hkuN-k024uDotyIEJnVZflIv8cs,234
-vsbasicvsrpp/sr_backbone_utils.py,sha256=cZg8eS3esWWuVhDOOMt7yiPp0F_HwG65cp4uWtqmzow,3075
-vsbasicvsrpp/upsample.py,sha256=W_dGBNJXuf7Ebc79CjNE2b0LwNjHMAi4LvTTuitCBLw,1557
+vsbasicvsrpp/basicvsr.py,sha256=GQu7_tvboDuUiQCHU1Tw6YIVMMtIp-1n0aQQ9kh4K9Y,864
+vsbasicvsrpp/basicvsr_net.py,sha256=kK9ZeVfhsgNIwZXZoPKFHAtrOVfnD785cyuVFK2tXP0,7873
+vsbasicvsrpp/basicvsr_plusplus_net.py,sha256=_kYZH5vwMA2p3QoPKynGvRBPiNY_CSUitDzjiGUAqiI,16157
+vsbasicvsrpp/flow_warp.py,sha256=jHa2MlrjxH1zoY884-hlMdj05rn-O6XIZZorJBNjSAo,2392
+vsbasicvsrpp/model_utils.py,sha256=hKX7syf6VNg2yIlU2N5cWh0RhK2J15rPsgc_hNzpoXY,1287
+vsbasicvsrpp/registry.py,sha256=4FOW2cTCCZMtiKCaIClSKWybmBjUmR_9CtrMitWpSoM,532
+vsbasicvsrpp/sr_backbone.py,sha256=wlHTk3I-VJRqzmILej6vItORfX5ht5k95cKn6Zo34DM,1952
+vsbasicvsrpp/upsample.py,sha256=WZXZnCB7geRKO-sExnOfcIMyOt5LyjcSY5HS6njQSkQ,1623
 vsbasicvsrpp/models/basicvsr_plusplus_c128n25_ntire_decompress_track1_20210223-7b2eba02.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vsbasicvsrpp/models/basicvsr_plusplus_c128n25_ntire_decompress_track2_20210314-eeae05e6.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vsbasicvsrpp/models/basicvsr_plusplus_c128n25_ntire_decompress_track3_20210304-6daf4a40.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vsbasicvsrpp/models/basicvsr_plusplus_c128n25_ntire_vsr_20210311-1ff35292.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vsbasicvsrpp/models/basicvsr_plusplus_c64n7_8x1_300k_vimeo90k_bd_20210305-ab315ab1.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vsbasicvsrpp/models/basicvsr_plusplus_c64n7_8x1_300k_vimeo90k_bi_20210305-4ef437e2.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vsbasicvsrpp/models/basicvsr_plusplus_c64n7_8x1_600k_reds4_20210217-db622b2f.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vsbasicvsrpp/models/basicvsr_plusplus_deblur_dvd-ecd08b7f.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vsbasicvsrpp/models/basicvsr_plusplus_deblur_gopro-3c5bb9b5.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vsbasicvsrpp/models/basicvsr_plusplus_denoise-28f6920c.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 vsbasicvsrpp/models/spynet_20210409-c6c1bd09.pth,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-vsbasicvsrpp-2.0.0.dist-info/METADATA,sha256=Tl7kglR5lswreEaIL4J02_0rKzxz-tI8zeA0IcupASU,14389
-vsbasicvsrpp-2.0.0.dist-info/WHEEL,sha256=Fd6mP6ydyRguakwUJ05oBE7fh2IPxgtDN9IwHJ9OqJQ,87
-vsbasicvsrpp-2.0.0.dist-info/licenses/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
-vsbasicvsrpp-2.0.0.dist-info/RECORD,,
+vsbasicvsrpp-2.1.0.dist-info/METADATA,sha256=e7gaZA8o_WHx9eenDd56WxcP6mz_yy6Kw69vIkpj6P4,14422
+vsbasicvsrpp-2.1.0.dist-info/WHEEL,sha256=y1bSCq4r5i4nMmpXeUJMqs3ipKvkZObrIXSvJHm1qCI,87
+vsbasicvsrpp-2.1.0.dist-info/licenses/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
+vsbasicvsrpp-2.1.0.dist-info/RECORD,,
```

