# Comparing `tmp/videosrc-0.1.6-py3-none-any.whl.zip` & `tmp/videosrc-0.1.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,25 +1,29 @@
-Zip file size: 86790 bytes, number of entries: 23
--rw-rw-r--  2.0 unx        0 b- defN 22-Oct-11 15:15 tests/__init__.py
--rw-rw-r--  2.0 unx     2815 b- defN 22-Oct-26 20:57 tests/test_html.py
--rw-rw-r--  2.0 unx     6428 b- defN 22-Oct-26 20:58 tests/test_mrss.py
--rw-rw-r--  2.0 unx    34148 b- defN 22-Oct-26 02:32 tests/test_odysee.py
--rw-rw-r--  2.0 unx    29432 b- defN 22-Oct-26 20:58 tests/test_peertube.py
--rw-rw-r--  2.0 unx    76420 b- defN 22-Oct-26 02:32 tests/test_rumble.py
--rw-rw-r--  2.0 unx   191649 b- defN 22-Oct-26 02:33 tests/test_timcast.py
--rw-rw-r--  2.0 unx      943 b- defN 22-Oct-26 19:10 videosrc/__init__.py
--rw-rw-r--  2.0 unx      859 b- defN 22-Oct-26 19:09 videosrc/__main__.py
--rw-rw-r--  2.0 unx     4041 b- defN 22-Oct-26 20:45 videosrc/utils.py
--rw-rw-r--  2.0 unx      429 b- defN 22-Oct-26 02:31 videosrc/crawlers/__init__.py
--rw-rw-r--  2.0 unx     3058 b- defN 22-Oct-26 20:56 videosrc/crawlers/html.py
--rw-rw-r--  2.0 unx     6504 b- defN 22-Oct-26 20:55 videosrc/crawlers/mrss.py
--rw-rw-r--  2.0 unx    11273 b- defN 22-Oct-26 20:51 videosrc/crawlers/odysee.py
--rw-rw-r--  2.0 unx     5182 b- defN 22-Oct-26 21:00 videosrc/crawlers/peertube.py
--rw-rw-r--  2.0 unx     3701 b- defN 22-Oct-26 20:48 videosrc/crawlers/rumble.py
--rw-rw-r--  2.0 unx     6403 b- defN 22-Oct-26 20:46 videosrc/crawlers/timcast.py
--rw-rw-r--  2.0 unx      612 b- defN 22-Oct-26 20:40 videosrc/models/__init__.py
--rw-rw-r--  2.0 unx     1066 b- defN 22-Oct-26 21:01 videosrc-0.1.6.dist-info/LICENSE
--rw-rw-r--  2.0 unx     1435 b- defN 22-Oct-26 21:01 videosrc-0.1.6.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 22-Oct-26 21:01 videosrc-0.1.6.dist-info/WHEEL
--rw-rw-r--  2.0 unx       15 b- defN 22-Oct-26 21:01 videosrc-0.1.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1826 b- defN 22-Oct-26 21:01 videosrc-0.1.6.dist-info/RECORD
-23 files, 388331 bytes uncompressed, 83872 bytes compressed:  78.4%
+Zip file size: 91788 bytes, number of entries: 27
+-rw-rw-r--  2.0 unx      311 b- defN 23-May-12 19:55 tests/__init__.py
+-rw-rw-r--  2.0 unx     2862 b- defN 23-May-17 02:05 tests/test_html.py
+-rw-rw-r--  2.0 unx     6475 b- defN 23-May-17 02:05 tests/test_mrss.py
+-rw-rw-r--  2.0 unx    34252 b- defN 23-May-17 02:12 tests/test_odysee.py
+-rw-rw-r--  2.0 unx    29900 b- defN 23-May-14 05:44 tests/test_peertube.py
+-rw-rw-r--  2.0 unx    76475 b- defN 23-May-14 05:44 tests/test_rumble.py
+-rw-rw-r--  2.0 unx   191646 b- defN 23-May-14 05:44 tests/test_timcast.py
+-rw-rw-r--  2.0 unx     1112 b- defN 23-May-14 20:05 tests/test_twitter.py
+-rw-rw-r--  2.0 unx     1685 b- defN 23-May-20 19:55 videosrc/__init__.py
+-rw-rw-r--  2.0 unx     1812 b- defN 23-May-15 15:31 videosrc/__main__.py
+-rw-rw-r--  2.0 unx      189 b- defN 23-May-14 21:39 videosrc/errors.py
+-rw-rw-r--  2.0 unx     5296 b- defN 23-May-15 15:50 videosrc/utils.py
+-rw-rw-r--  2.0 unx      500 b- defN 23-May-11 21:33 videosrc/crawlers/__init__.py
+-rw-rw-r--  2.0 unx     2256 b- defN 23-May-17 02:04 videosrc/crawlers/base.py
+-rw-rw-r--  2.0 unx     3565 b- defN 23-May-17 02:02 videosrc/crawlers/html.py
+-rw-rw-r--  2.0 unx     6869 b- defN 23-May-17 01:59 videosrc/crawlers/mrss.py
+-rw-rw-r--  2.0 unx    12880 b- defN 23-May-17 02:12 videosrc/crawlers/odysee.py
+-rw-rw-r--  2.0 unx     5709 b- defN 23-May-17 02:02 videosrc/crawlers/peertube.py
+-rw-rw-r--  2.0 unx     3804 b- defN 23-May-15 14:56 videosrc/crawlers/rumble.py
+-rw-rw-r--  2.0 unx     6783 b- defN 23-May-17 02:02 videosrc/crawlers/timcast.py
+-rw-rw-r--  2.0 unx     3121 b- defN 23-May-15 14:41 videosrc/crawlers/twitter.py
+-rw-rw-r--  2.0 unx      626 b- defN 23-May-14 05:19 videosrc/models/__init__.py
+-rw-rw-r--  2.0 unx     1066 b- defN 23-May-20 20:05 videosrc-0.1.7.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     2011 b- defN 23-May-20 20:05 videosrc-0.1.7.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-20 20:05 videosrc-0.1.7.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       15 b- defN 23-May-20 20:05 videosrc-0.1.7.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2149 b- defN 23-May-20 20:05 videosrc-0.1.7.dist-info/RECORD
+27 files, 403461 bytes uncompressed, 88382 bytes compressed:  78.1%
```

## zipnote {}

```diff
@@ -15,26 +15,35 @@
 
 Filename: tests/test_rumble.py
 Comment: 
 
 Filename: tests/test_timcast.py
 Comment: 
 
+Filename: tests/test_twitter.py
+Comment: 
+
 Filename: videosrc/__init__.py
 Comment: 
 
 Filename: videosrc/__main__.py
 Comment: 
 
+Filename: videosrc/errors.py
+Comment: 
+
 Filename: videosrc/utils.py
 Comment: 
 
 Filename: videosrc/crawlers/__init__.py
 Comment: 
 
+Filename: videosrc/crawlers/base.py
+Comment: 
+
 Filename: videosrc/crawlers/html.py
 Comment: 
 
 Filename: videosrc/crawlers/mrss.py
 Comment: 
 
 Filename: videosrc/crawlers/odysee.py
@@ -45,26 +54,29 @@
 
 Filename: videosrc/crawlers/rumble.py
 Comment: 
 
 Filename: videosrc/crawlers/timcast.py
 Comment: 
 
+Filename: videosrc/crawlers/twitter.py
+Comment: 
+
 Filename: videosrc/models/__init__.py
 Comment: 
 
-Filename: videosrc-0.1.6.dist-info/LICENSE
+Filename: videosrc-0.1.7.dist-info/LICENSE
 Comment: 
 
-Filename: videosrc-0.1.6.dist-info/METADATA
+Filename: videosrc-0.1.7.dist-info/METADATA
 Comment: 
 
-Filename: videosrc-0.1.6.dist-info/WHEEL
+Filename: videosrc-0.1.7.dist-info/WHEEL
 Comment: 
 
-Filename: videosrc-0.1.6.dist-info/top_level.txt
+Filename: videosrc-0.1.7.dist-info/top_level.txt
 Comment: 
 
-Filename: videosrc-0.1.6.dist-info/RECORD
+Filename: videosrc-0.1.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tests/__init__.py

```diff
@@ -0,0 +1,20 @@
+00000000: 696d 706f 7274 206c 6f67 6769 6e67 0a0a  import logging..
+00000010: 4c4f 4747 4552 203d 206c 6f67 6769 6e67  LOGGER = logging
+00000020: 2e67 6574 4c6f 6767 6572 2829 0a4c 4f47  .getLogger().LOG
+00000030: 4745 522e 6164 6448 616e 646c 6572 286c  GER.addHandler(l
+00000040: 6f67 6769 6e67 2e53 7472 6561 6d48 616e  ogging.StreamHan
+00000050: 646c 6572 2829 290a 4c4f 4747 4552 2e73  dler()).LOGGER.s
+00000060: 6574 4c65 7665 6c28 6c6f 6767 696e 672e  etLevel(logging.
+00000070: 4552 524f 5229 0a0a 6672 6f6d 2074 6573  ERROR)..from tes
+00000080: 7473 2e74 6573 745f 6874 6d6c 2069 6d70  ts.test_html imp
+00000090: 6f72 7420 2a0a 6672 6f6d 2074 6573 7473  ort *.from tests
+000000a0: 2e74 6573 745f 6d72 7373 2069 6d70 6f72  .test_mrss impor
+000000b0: 7420 2a0a 6672 6f6d 2074 6573 7473 2e74  t *.from tests.t
+000000c0: 6573 745f 6f64 7973 6565 2069 6d70 6f72  est_odysee impor
+000000d0: 7420 2a0a 6672 6f6d 2074 6573 7473 2e74  t *.from tests.t
+000000e0: 6573 745f 7065 6572 7475 6265 2069 6d70  est_peertube imp
+000000f0: 6f72 7420 2a0a 6672 6f6d 2074 6573 7473  ort *.from tests
+00000100: 2e74 6573 745f 7275 6d62 6c65 2069 6d70  .test_rumble imp
+00000110: 6f72 7420 2a0a 6672 6f6d 2074 6573 7473  ort *.from tests
+00000120: 2e74 6573 745f 7469 6d63 6173 7420 696d  .test_timcast im
+00000130: 706f 7274 202a 0a                        port *.
```

## tests/test_html.py

```diff
@@ -72,22 +72,23 @@
         self.server = TestServer()
         self.server.start()
 
     def tearDown(self):
         self.server.stop()
 
     async def test_login(self):
-        await self.crawler.login('foobar', 'quux')
+        await self.crawler.login(
+            self.server.url(), username='foobar', password='quux')
         self.assertEqual({
             'headers': {'Authorization': 'Zm9vYmFyOnF1dXg='}
         }, self.crawler.auth)
 
     async def test_crawl(self):
         channel, videos = await self.crawler.crawl(self.server.url())
-        videos = [v async for v, s in videos]
+        videos = [v async for v in videos]
         self.assertEqual('Directory listing for /', channel.name)
         self.assertEqual(5, len(videos))
         self.assertIsNotNone(videos[0].extern_id)
         self.assertEqual('For Bigger Blazes', videos[0].title)
         self.assertEqual(719760, videos[0].duration)
         self.assertEqual(1, len(videos[0].sources))
         self.assertEqual(1280, videos[0].sources[0].width)
```

## tests/test_mrss.py

```diff
@@ -92,22 +92,23 @@
 
     def tearDown(self):
         self.server.stop()
         for f in self.files:
             f.close()
 
     async def test_login(self):
-        await self.crawler.login('foobar', 'quux')
+        await self.crawler.login(
+            self.server.url(), username='foobar', password='quux')
         self.assertEqual({
             'headers': {'Authorization': 'Zm9vYmFyOnF1dXg='}
         }, self.crawler.auth)
 
     async def test_crawl(self):
         channel, videos = await self.crawler.crawl(self.server.url('/'))
-        videos = [v async for v, s in videos]
+        videos = [v async for v in videos]
         self.assertEqual('Calm Meditation', channel.name)
         self.assertEqual(2, len(videos))
         self.assertEqual('Shade', videos[0].title)
         self.assertEqual(
             'Quiet the mind, and the soul will speak. - Ma Jaya Sati Bhagavati',
             videos[0].description)
         self.assertListEqual(['All', 'Trail'], videos[0].tags)
```

## tests/test_odysee.py

```diff
@@ -2,14 +2,15 @@
 
 from responses.registries import OrderedRegistry
 from responses_server import ResponsesServer
 
 from videosrc.crawlers.odysee import OdyseeCrawler
 
 
+AUTH = {'auth_token': 'foobar'}
 RSP0 = {'id': 0,
  'jsonrpc': '2.0',
  'result': {'lbry://@timcast#c': {'address': 'bMyDG3xgFUK4fao4dU8jq1THhEGhJr5z55',
                                   'amount': '0.005',
                                   'canonical_url': 'lbry://@timcast#c',
                                   'claim_id': 'c9da929d12afe6066acc89eb044b552f0d63782a',
                                   'claim_op': 'update',
@@ -455,28 +456,29 @@
 
 class OdyseeTestCase(IsolatedAsyncioTestCase):
     def setUp(self):
         self.server = ResponsesServer(
             responses_kwargs={'registry': OrderedRegistry})
         self.server.start()
         self.crawler = OdyseeCrawler(api_url=self.server.url())
+        self.server.post(self.server.url('/user/new'), json=AUTH)
         self.server.post(self.server.url(), json=RSP0)
         self.server.post(self.server.url(), json=RSP1)
         self.server.post(self.server.url(), json=RSP2)
         self.server.post(self.server.url(), json=RSP3)
 
     def tearDown(self):
         self.server.stop()
 
     async def test_login(self):
-        pass #  await self.crawler.login()
+        await self.crawler.login(self.server.url())
 
     async def test_crawl(self):
         channel, videos = await self.crawler.crawl('https://odysee.com/@timcast:c')
-        videos = [v async for v, s in videos]
+        videos = [v async for v in videos]
         self.assertEqual('timcast', channel.name)
         self.assertEqual(2, len(videos))
         self.assertEqual(
             'Elon Musk NUKED Ukrainian Gov FROM ORBIT SHUTTERING Starlink After They Told Him To F OFF',
             videos[0].title)
         self.assertEqual(1, len(videos[0].sources))
         self.assertEqual(
```

## tests/test_peertube.py

```diff
@@ -1,13 +1,13 @@
-from unittest import IsolatedAsyncioTestCase
+from unittest import TestCase, IsolatedAsyncioTestCase
 
 from responses.registries import OrderedRegistry
 from responses_server import ResponsesServer
 
-from videosrc.crawlers.peertube import PeerTubeCrawler
+from videosrc.crawlers.peertube import PeerTubeCrawler, maybe_parse_date
 
 
 RSP0 = {'avatars': [],
  'banners': [],
  'createdAt': '2022-07-31T00:50:59.621Z',
  'description': None,
  'displayName': "Ben's channel",
@@ -392,40 +392,57 @@
  'url': 'http://cesium.tv/videos/watch/5c797630-f4d3-4229-8ac4-f81c148a6256',
  'uuid': '5c797630-f4d3-4229-8ac4-f81c148a6256',
  'viewers': 0,
  'views': 0,
  'waitTranscoding': True}
 
 
+
+class PeerTubeDateParseTestCase(TestCase):
+    def test_parse(self):
+        dt = maybe_parse_date('2022-07-31T00:54:44.991Z')
+        self.assertEqual(dt.year, 2022)
+        self.assertEqual(dt.month, 7)
+        self.assertEqual(dt.day, 31)
+        self.assertEqual(dt.hour, 0)
+        self.assertEqual(dt.minute, 54)
+        self.assertEqual(dt.second, 44)
+
+
 class PeerTubeTestCase(IsolatedAsyncioTestCase):
     def setUp(self):
         self.server = ResponsesServer(
             responses_kwargs={'registry': OrderedRegistry})
         self.server.start()
         self.crawler = PeerTubeCrawler()
         self.server.get(
-            self.server.url('/api/v1/video-channels/btimby_channel@cesium.tv:80'),
+            self.server.url(
+                '/api/v1/video-channels/btimby_channel@cesium.tv:80'),
             json=RSP0)
         self.server.get(
-            self.server.url('/api/v1/video-channels/btimby_channel@cesium.tv:80/videos'),
+            self.server.url(
+                '/api/v1/video-channels/btimby_channel@cesium.tv:80/videos'),
             json=RSP1)
         self.server.get(
-            self.server.url('/api/v1/videos/iNid2sMo2cZQrJBnipxiXs'), json=RSP2)
+            self.server.url(
+                '/api/v1/videos/iNid2sMo2cZQrJBnipxiXs'), json=RSP2)
         self.server.get(
-            self.server.url('/api/v1/videos/cqiZm4MvzSkrFhb5THHRgU'), json=RSP3)
+            self.server.url(
+                '/api/v1/videos/cqiZm4MvzSkrFhb5THHRgU'), json=RSP3)
 
     def tearDown(self):
         self.server.stop()
 
     async def test_login(self):
         pass #  await self.crawler.login()
 
     async def test_peertube(self):
-        channel, videos = await self.crawler.crawl(self.server.url('/c/btimby_channel@cesium.tv:80'))
-        videos = [v async for v, s in videos]
+        channel, videos = await self.crawler.crawl(
+            self.server.url('/c/btimby_channel@cesium.tv:80'))
+        videos = [v async for v in videos]
         self.assertEqual('btimby_channel@cesium.tv:80', channel.name)
         self.assertEqual(2, len(videos))
         self.assertEqual('ElephantsDream',videos[0].title)
         self.assertEqual(3, len(videos[0].sources))
         self.assertEqual(
             'http://cesium.tv/static/webseed/04f56e6b-027d-411f-ac8d-7871d66f7863-720.mp4',
             videos[0].sources[0].url)
```

## tests/test_rumble.py

```diff
@@ -374,16 +374,17 @@
             body=RSP2)
         self.crawler = RumbleCrawler()
 
     def tearDown(self):
         self.server.stop()
 
     async def test_login(self):
-        await self.crawler.login()
+         with self.assertRaises(NotImplementedError):
+            await self.crawler.login()
 
     async def test_crawl(self):
         channel, videos = await self.crawler.crawl(
             self.server.url('/user/vivafrei/'))
-        videos = [v async for v, s in videos]
+        videos = [v async for v in videos]
         self.assertEqual('vivafrei', channel.name)
         self.assertEqual(1, len(videos))
         self.assertEqual(4, len(videos[0].sources))
```

## tests/test_timcast.py

```diff
@@ -11961,19 +11961,18 @@
 0002eb80: 7261 776c 2873 656c 6629 3a0a 2020 2020  rawl(self):.    
 0002eb90: 2020 2020 6368 616e 6e65 6c2c 2076 6964      channel, vid
 0002eba0: 656f 7320 3d20 6177 6169 7420 7365 6c66  eos = await self
 0002ebb0: 2e63 7261 776c 6572 2e63 7261 776c 2873  .crawler.crawl(s
 0002ebc0: 656c 662e 7365 7276 6572 2e75 726c 2829  elf.server.url()
 0002ebd0: 290a 2020 2020 2020 2020 7669 6465 6f73  ).        videos
 0002ebe0: 203d 205b 7620 6173 796e 6320 666f 7220   = [v async for 
-0002ebf0: 762c 2073 2069 6e20 7669 6465 6f73 5d0a  v, s in videos].
-0002ec00: 2020 2020 2020 2020 7365 6c66 2e61 7373          self.ass
-0002ec10: 6572 7445 7175 616c 2827 4d65 6d62 6572  ertEqual('Member
-0002ec20: 7320 4f6e 6c79 3a20 5469 6d63 6173 7420  s Only: Timcast 
-0002ec30: 4952 4c27 2c20 6368 616e 6e65 6c2e 6e61  IRL', channel.na
-0002ec40: 6d65 290a 2020 2020 2020 2020 7365 6c66  me).        self
-0002ec50: 2e61 7373 6572 7445 7175 616c 2831 2c20  .assertEqual(1, 
-0002ec60: 6c65 6e28 7669 6465 6f73 2929 0a20 2020  len(videos)).   
-0002ec70: 2020 2020 2073 656c 662e 6173 7365 7274       self.assert
-0002ec80: 4571 7561 6c28 352c 206c 656e 2876 6964  Equal(5, len(vid
-0002ec90: 656f 735b 305d 2e73 6f75 7263 6573 2929  eos[0].sources))
-0002eca0: 0a                                       .
+0002ebf0: 7620 696e 2076 6964 656f 735d 0a20 2020  v in videos].   
+0002ec00: 2020 2020 2073 656c 662e 6173 7365 7274       self.assert
+0002ec10: 4571 7561 6c28 274d 656d 6265 7273 204f  Equal('Members O
+0002ec20: 6e6c 793a 2054 696d 6361 7374 2049 524c  nly: Timcast IRL
+0002ec30: 272c 2063 6861 6e6e 656c 2e6e 616d 6529  ', channel.name)
+0002ec40: 0a20 2020 2020 2020 2073 656c 662e 6173  .        self.as
+0002ec50: 7365 7274 4571 7561 6c28 312c 206c 656e  sertEqual(1, len
+0002ec60: 2876 6964 656f 7329 290a 2020 2020 2020  (videos)).      
+0002ec70: 2020 7365 6c66 2e61 7373 6572 7445 7175    self.assertEqu
+0002ec80: 616c 2835 2c20 6c65 6e28 7669 6465 6f73  al(5, len(videos
+0002ec90: 5b30 5d2e 736f 7572 6365 7329 290a       [0].sources)).
```

### html2text {}

```diff
@@ -530,10 +530,10 @@
 ( self.server.url(), headers={'Content-Type': 'text/html'}, body=RSP0)
 iframe_url = self.server.url('/embed/v1m7kuc/') self.server.get
 ( self.server.url('/members-area/seth-weathers-megan-hansen-uncensored-show-
 crew-talks-nutrition-and-plastics-making-men-weak/'), headers={'Content-Type':
 'text/html'}, body=RSP1.replace('%%URL%%', iframe_url)) self.server.get
 (iframe_url, headers={'Content-Type': 'text/html'}, body=RSP2) def tearDown
 (self): self.server.stop() async def test_crawl(self): channel, videos = await
-self.crawler.crawl(self.server.url()) videos = [v async for v, s in videos]
+self.crawler.crawl(self.server.url()) videos = [v async for v in videos]
 self.assertEqual('Members Only: Timcast IRL', channel.name) self.assertEqual(1,
 len(videos)) self.assertEqual(5, len(videos[0].sources))
```

## videosrc/__init__.py

```diff
@@ -1,13 +1,14 @@
 import asyncio
 import inspect
 
 import videosrc.crawlers
 from videosrc.crawlers.html import HTMLCrawler
 from videosrc.utils import iter_sync
+from videosrc.errors import AuthenticationError
 
 
 def detect_crawler(url):
     crawlers = []
 
     for cls in videosrc.crawlers.__dict__.values():
         if not inspect.isclass(cls):
@@ -19,18 +20,46 @@
     # Set a default crawler if none match.
     if not crawlers:
         crawlers.append(HTMLCrawler)
 
     return crawlers
 
 
-async def crawl(url, state=None, credentials=None):
-    crawler = detect_crawler(url)[0](state=state)
-    if credentials:
-        await crawler.login(url, **credentials)
-    return await crawler.crawl(url)
+async def login(url, **kwargs):
+    init_kwargs = {
+        'state': kwargs.pop('state', None),
+        'save_state': kwargs.pop('save_state', None),
+        'proxy': kwargs.pop('proxy', None),
+    }    
+
+    crawler_klass = detect_crawler(url)[0]
+    crawler = crawler_klass(**init_kwargs)
+
+    try:
+        return await crawler.login(url, **kwargs)
+
+    except Exception:
+        raise AuthenticationError('Failure logging in')
+
+
+async def crawl(url, **kwargs):
+    init_kwargs = {
+        'state': kwargs.pop('state', None),
+        'save_state': kwargs.pop('save_state', None),
+        'proxy': kwargs.pop('proxy', None),
+    }    
+
+    crawler_klass = detect_crawler(url)[0]
+    crawler = crawler_klass(**init_kwargs)
+
+    return await crawler.crawl(url, **kwargs)
+
+
+def login_sync(*args, **kwargs):
+    loop = asyncio.get_event_loop()
+    return loop.run_until_complete(login(*args, **kwargs))
 
 
 def crawl_sync(*args, **kwargs):
     loop = asyncio.get_event_loop()
     channel, videos = loop.run_until_complete(crawl(*args, **kwargs))
     return channel, iter_sync(videos, loop=loop)
```

## videosrc/__main__.py

```diff
@@ -1,40 +1,76 @@
-import asyncio
 import logging
+import pickle
 
+from urllib.parse import urlparse
 from pprint import pprint
-from optparse import OptionParser
+from argparse import ArgumentParser, ArgumentTypeError
 
 from videosrc import crawl_sync
 
 
 LOGGER = logging.getLogger('vidsrc')
 LOGGER.setLevel(logging.DEBUG)
 LOGGER.addHandler(logging.StreamHandler())
 
 
-def main(options, args):
-    url, credentials = args[0], {}
+def url(s):
+    urlp = urlparse(s)
+    if all((urlp.scheme, urlp.netloc)):
+        return s
+    raise ArgumentTypeError('Invalid URL')
 
-    if options.username:
-        credentials['username'] = options.username
-        
-    if options.password:
-        credentials['password'] = options.password
 
-    channel, videos = crawl_sync(url, credentials=credentials)
+def save_state_factory(path):
+    def _save_state(state):
+        LOGGER.info('Saving state to file %s', path)
+        with open(path) as f:
+            pickle.dump(f, state)
+    return _save_state
+
+
+def load_state(path):
+    LOGGER.info('Loading state from file %s', path)
+    with open(path, 'rb') as f:
+        return pickle.load(f)
+
+
+def main(args):
+    credentials = {}
+    kwargs = {'credentials': credentials}
+    url = args.url
+
+    if args.username:
+        credentials['username'] = args.username
+
+    if args.password:
+        credentials['password'] = args.password
+
+    if args.state:
+        kwargs['state'] = load_state(args.state)
+        kwargs['save_state'] = save_state_factory(args.state)
+
+    if args.proxy:
+        kwargs['proxy'] = args.proxy
+
+    channel, videos = crawl_sync(url, **kwargs)
 
     pprint(channel)
-    for video, state in videos:
+    for video in videos:
         pprint(video)
 
 
 if __name__ == '__main__':
-    parser = OptionParser()
+    parser = ArgumentParser()
 
-    parser.add_option(
+    parser.add_argument('url', type=url, help='URL to collect video from')
+    parser.add_argument(
         '-u', '--username', help='Username')
-    parser.add_option(
+    parser.add_argument(
         '-p', '--password', help='Password')
+    parser.add_argument(
+        '-P', '--proxy', type=url, help='Proxy server url')
+    parser.add_argument(
+        '-s', '--state', help='Path at which to save crawl state')
 
-    options, args = parser.parse_args()
-    main(options, args)
+    args = parser.parse_args()
+    main(args)
```

## videosrc/utils.py

```diff
@@ -1,25 +1,29 @@
 import re
 import types
 import asyncio
+import logging
 
 from base64 import b64encode
 from numbers import Number
 from fractions import Fraction
 from datetime import datetime
 from email.utils import parsedate_to_datetime
 from io import BytesIO
-from urllib.parse import urljoin, quote, urlparse
-from base64 import b64encode
+from urllib.parse import quote, urlparse
 from os.path import splitext, basename
 from hashlib import md5
+from mimetypes import guess_type
 
 import av
 import requests
-from bs4 import BeautifulSoup
+
+
+LOGGER = logging.getLogger(__name__)
+LOGGER.addHandler(logging.NullHandler())
 
 
 def md5sum(s):
     try:
         s = s.encode()
     except UnicodeEncodeError:
         pass
@@ -28,17 +32,18 @@
 
 def get_tag_text(o, name):
     tag = o.find(name)
     return tag and tag.text
 
 
 def basic_auth(username, password):
+    auth = f'{username}:{password}'.encode()
     return {
         'headers': {
-            'Authorization': b64encode(f'{username}:{password}'.encode()).decode(),
+            'Authorization': b64encode(auth).decode(),
         },
     }
 
 
 def isiterable(o):
     try:
         iter(o)
@@ -70,23 +75,27 @@
     return d
 
 
 def url2title(url):
     urlp = urlparse(url)
     fn = splitext(basename(urlp.path))[0]
     wupper = [
-        s for s in [s.strip() for s in re.split('(?=[A-Z])', fn)] if s
+        s for s in [s.strip() for s in re.split(r'(?=[A-Z])', fn)] if s
     ]
     wnword = [
-        s for s in [s.strip() for s in re.split('[\s\-_=+]+', fn)] if s
+        s for s in [s.strip() for s in re.split(r'[\s\-_=+]+', fn)] if s
     ]
     words = wupper if len(wupper) > len(wnword) else wnword
     return ' '.join(words).title()
 
 
+def url2mime(url):
+    return guess_type(urlparse(url).path)[0]
+
+
 def iter_sync(f, loop=None):
     # NOTE: This code converts from async generator to sync generator.
     loop = loop or asyncio.get_event_loop()
     ait = f.__aiter__()
 
     async def get_next():
         try:
@@ -100,64 +109,116 @@
         done, obj = loop.run_until_complete(get_next())
         if done:
             break
 
         yield obj
 
 
+async def aenumerate(seq, start=0):
+    n = start
+    async for e in seq:
+        yield n, e
+        n += 1
+
+
 class MediaInfo:
     """
     Decodes remote video file.
 
     Uses lazy-loading properties to do as little work as is necessary.
     """
     def __init__(self, url):
         self.url = url
         self._video = None
         self._frame = None
         self._headers = None
 
+    def __enter__(self):
+        return self
+
+    def __exit__(self, *args):
+        self.close()
+
     @property
     def headers(self):
         if self._headers is None:
             # NOTE: We have not yet fetched headers, do a quick HEAD request.
             with requests.head(self.url) as r:
                 self._headers = r.headers
         return self._headers
 
     @property
     def video(self):
         if self._video is None:
             with requests.get(self.url, stream=True) as r:
                 self._headers = r.headers
-                self._video = av.open(r.raw)
+                try:
+                    self._video = av.open(r.raw)
+                except Exception as e:
+                    LOGGER.warning(e, exc_info=True)
         return self._video
 
     @property
     def stream(self):
-        return self.video.streams.video[0]
+        try:
+            return self.video.streams.video[0]
+        except IndexError as e:
+            LOGGER.warning(e, exc_info=True)
+            return None
 
     @property
     def frame(self):
         if self._frame is None:
             stream = self.stream
             stream.codec_context.skip_frame = "NONKEY"
             self._frame = next(self.video.decode(stream))
         return self._frame
 
     @property
     def size(self):
         return int(self.headers.get('Content-Length', 0))
 
+    @property
+    def mime(self):
+        return self.headers.get('Content-Type')
+
+    @property
+    def width(self):
+        try:
+            return self.stream.width
+        except AttributeError:
+            return None
+
+    @property
+    def height(self):
+        try:
+            return self.stream.height
+        except AttributeError:
+            return None
+
+    @property
+    def duration(self):
+        try:
+            return self.stream.duration
+        except AttributeError:
+            return None
+
+    @property
+    def fps(self):
+        try:
+            return self.stream.guessed_rate
+        except AttributeError:
+            return None
+
     def last_modified(self, default=datetime.now):
         try:
             last_modified = self.headers['Last-Modified']
         except KeyError:
             return default() if callable(default) else default
-        parsedate_to_datetime(last_modified)
+        return parsedate_to_datetime(last_modified)
 
     def poster(self):
         poster = BytesIO()
         self.frame.to_image().save(poster, format='png')
         return f'data:image/png;base64,{quote(b64encode(poster.getvalue()))}'
 
     def close(self):
```

## videosrc/crawlers/__init__.py

```diff
@@ -1,12 +1,13 @@
 from videosrc.crawlers.timcast import TimcastCrawler
 from videosrc.crawlers.peertube import PeerTubeCrawler
 from videosrc.crawlers.rumble import RumbleCrawler
 from videosrc.crawlers.html import HTMLCrawler
 from videosrc.crawlers.mrss import MRSSCrawler
 from videosrc.crawlers.odysee import OdyseeCrawler
+from videosrc.crawlers.twitter import TwitterCrawler
 
 
 __all__ = [
     'TimcastCrawler', 'PeerTubeCrawler', 'RumbleCrawler', 'HTMLCrawler',
-    'MRSSCrawler', 'OdyseeCrawler',
+    'MRSSCrawler', 'OdyseeCrawler', 'TwitterCrawler',
 ]
```

## videosrc/crawlers/html.py

```diff
@@ -1,89 +1,112 @@
 import logging
 
 from urllib.parse import urljoin
-from hashlib import md5
 from email.utils import parsedate_to_datetime
 
 from aiohttp.hdrs import METH_GET
 from aiohttp_scraper import ScraperSession
 from bs4 import BeautifulSoup
 
-from videosrc.models import Channel, Video, VideoSource
 from videosrc.utils import dict_repr, url2title, MediaInfo, basic_auth, md5sum
+from videosrc.crawlers.base import Crawler
 
 
 LOGGER = logging.getLogger(__name__)
 LOGGER.addHandler(logging.NullHandler())
 
 
-class HTMLCrawler:
-    def __init__(self, state=None, ChannelModel=Channel, VideoModel=Video,
-                 VideoSourceModel=VideoSource):
+class HTMLCrawler(Crawler):
+    auth_fields = {
+        'username': 'String',
+        'password': 'String',
+    }
+
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
         self.auth = {}
-        self.state = state
-        self.ChannelModel = ChannelModel
-        self.VideoModel = VideoModel
-        self.VideoSourceModel = VideoSourceModel
 
     @staticmethod
     def check_url(url):
         # NOTE: This crawler is too generalized to claim a url. It will be
         # chosen as a default if no other crawler claims a url.
         return False
 
-    async def login(self, username, password):
+    async def login(self, url, **kwargs):
+        try:
+            username = kwargs['username']
+            password = kwargs['password']
+        except KeyError:
+            LOGGER.warning('No credentials, skipping login')
+            return
+
         self.auth = basic_auth(username, password)
 
     async def _iter_videos(self, url, soup):
         for a in soup.find_all('a'):
             # NOTE: Link might be absolute.
             href = urljoin(url, a['href'])
-            info = MediaInfo(href)
             guid = md5sum(href)
-            source = self.VideoSourceModel(
-                extern_id=guid,
-                width=info.frame.width,
-                height=info.frame.height,
-                fps=info.stream.guessed_rate,
-                size=info.size,
-                url=href,
-                original={
-                    'url': href,
-                    'video': dict_repr(info.video),
-                },
-            )
-            video = self.VideoModel(
-                extern_id=guid,
-                title=url2title(href),
-                poster=info.poster(),
-                duration=info.stream.duration,
-                published=info.last_modified(),
-                sources=[source],
-                original={
-                    'url': href,
-                    'tag': str(a),
-                    'headers': dict(info.headers),
-                },
-            )
-            yield video, self.state
+            with MediaInfo(href) as info:
+                source = self.VideoSourceModel(
+                    extern_id=guid,
+                    width=info.width,
+                    height=info.height,
+                    fps=info.fps,
+                    size=info.size,
+                    url=href,
+                    mime=info.mime,
+                    original={
+                        'url': href,
+                        'video': dict_repr(info.video),
+                    },
+                )
+                video = self.VideoModel(
+                    extern_id=guid,
+                    title=url2title(href),
+                    poster=info.poster(),
+                    duration=info.duration,
+                    published=info.last_modified(),
+                    sources=[source],
+                    original={
+                        'url': href,
+                        'tag': str(a),
+                        'headers': dict(info.headers),
+                    },
+                )
+
+            try:
+                yield video
+
+            except Exception as e:
+                LOGGER.exception(e)
+
+    async def crawl(self, url, **kwargs):
+        await self.login(url, **kwargs)
 
-    async def crawl(self, url, options=None):
         async with ScraperSession() as s:
-            r = await s._request(METH_GET, url, **self.auth)
+            r = await s._request(METH_GET, url, proxy=self._proxy, **self.auth)
             try:
-                self.state = {
+                state = {
                     'Last-Modified': parsedate_to_datetime(
                         r.headers['Last-Modified']),
                     'Content-Length': int(r.headers['Content-Length']),
                 }
             except KeyError as e:
                 LOGGER.warning(
-                    'Could not read header: %s, cannot save state', e.args[0])
+                    'Could not read header: %s, unknown state', e.args[0])
+
+            else:
+                if self._state == state:
+                    LOGGER.info('Matching state, no updates')
+                    return
+                else:
+                    self._state = state
+
             soup = BeautifulSoup(await r.text(), 'html.parser')
             title = soup.find('title').text
             channel = self.ChannelModel(
                 extern_id=md5sum(url),
                 name=title,
                 url=url,
             )
-            return channel, self._iter_videos(url, soup)
+            return channel, self.iter_videos(url, soup, **kwargs)
```

## videosrc/crawlers/mrss.py

```diff
@@ -1,131 +1,165 @@
 import logging
 
 from email.utils import parsedate_to_datetime
-from itertools import chain
 from urllib.parse import urlparse
 
 from aiohttp.hdrs import METH_GET
 from aiohttp_scraper import ScraperSession
 from bs4 import BeautifulSoup
 
-from videosrc.models import Channel, Video, VideoSource
-from videosrc.utils import MediaInfo, basic_auth, get_tag_text, md5sum
+from videosrc.models import Video, VideoSource
+from videosrc.utils import MediaInfo, get_tag_text, md5sum
+from videosrc.crawlers.html import HTMLCrawler
 
 
 LOGGER = logging.getLogger(__name__)
 LOGGER.addHandler(logging.NullHandler())
 
 
-class MRSSCrawler:
-    def __init__(self, state=None, ChannelModel=Channel, VideoModel=Video,
-                 VideoSourceModel=VideoSource):
-        self.state = state
-        self.ChannelModel = ChannelModel
-        self.VideoModel = VideoModel
-        self.VideoSourceModel = VideoSourceModel
-
-    async def login(self, username, password):
-        self.auth = basic_auth(username, password)
+class MRSSCrawler(HTMLCrawler):
+    auth_fields = {
+        'username': 'String',
+        'password': 'String',
+    }
 
     @staticmethod
     def check_url(url):
         urlp = urlparse(url)
         return urlp.path.endswith('.mrss')
 
 # <item>
 #     <title>Shade</title>
 #     <pubDate>Mon, 23 Oct 2017 00:00:00 -0700</pubDate>
 #     <link>http://sample-firetv-web-app.s3-website-us-west-2.amazonaws.com/shade/</link>
-#     <description>Quiet the mind, and the soul will speak. - Ma Jaya Sati Bhagavati</description>
-#     <guid isPermaLink="false">http://sample-firetv-web-app.s3-website-us-west-2.amazonaws.com/shade/</guid>
+#     <description>Quiet the mind, and the soul will speak. - Ma Jaya Sati
+#       Bhagavati</description>
+#     <guid isPermaLink="false">
+#       http://sample-firetv-web-app.s3-website-us-west-2.amazonaws.com/shade/
+#     </guid>
 #     <media:category>All</media:category>
 #     <media:category>Trail</media:category>
-#     <media:content url="http://d1nixf144dcz0j.cloudfront.net/shade.mp4" language="en-us" fileSize="37000000" duration="120.0" medium="video" isDefault="true">
+#     <media:content url="http://d1nixf144dcz0j.cloudfront.net/shade.mp4"
+#       language="en-us" fileSize="37000000" duration="120.0" medium="video"
+#       isDefault="true">
 #     <media:title type="plain">Shade</media:title>
-#     <media:description type="html">Quiet the mind, and the soul will speak. - Ma Jaya Sati Bhagavati</media:description>
-#     <media:thumbnail url="http://sample-firetv-web-app.s3-website-us-west-2.amazonaws.com/images/thumbs/shade.jpg" />
+#     <media:description type="html">Quiet the mind, and the soul will speak.
+#       - Ma Jaya Sati Bhagavati</media:description>
+#     <media:thumbnail
+#       url="http://sample-firetv-web-app.s3-website-us-west-2.amazonaws.com
+#            /images/thumbs/shade.jpg"
+#     />
 #     <media:credit role="author" scheme="urn:ebu">Tom Johnson</media:credit>
 #     <media:copyright url="https://creativecommons.org/licenses/by/4.0/" />
 #     </media:content>
 # </item>
     async def _iter_videos(self, soup):
         for item in soup.find_all('item'):
             content = item.find('media:content')
             url = content['url']
-            info = MediaInfo(url)
             guid = item.guid.text or md5sum(url)
             title = get_tag_text(item, 'media:title') or \
-                    get_tag_text(item, 'title')
+                get_tag_text(item, 'title')
             description = get_tag_text(item, 'media:description') or \
-                            get_tag_text(item, 'description')
+                get_tag_text(item, 'description')
             poster = get_tag_text(item, 'media:thumbnail')
             published = parsedate_to_datetime(item.pubDate.text)
             keywords = get_tag_text(item, 'media:keywords')
             if keywords:
                 keywords = [s.strip() for s in keywords.split(',')]
             else:
                 keywords = []
             keywords.extend([
                 t.text for t in item.find_all('media:category')
             ])
-            source = VideoSource(
-                extern_id=guid,
-                width=info.frame.width,
-                height=info.frame.height,
-                fps=info.stream.guessed_rate,
-                size=int(content.get('fileSize', info.size)),
-                url=url,
-                original={},
-            )
-            video = Video(
-                extern_id=guid,
-                title=title,
-                description=description,
-                poster=poster,
-                duration=float(content.get('duration', info.stream.duration)),
-                published=published,
-                tags=keywords,
-                sources=[source],
-                original={
-                    'url': url,
-                    'headers': dict(info.headers),
-                    'tag': str(item),
-                },
-            )
-            yield video, self.state
+            with MediaInfo(url) as info:
+                source = VideoSource(
+                    extern_id=guid,
+                    width=info.width,
+                    height=info.height,
+                    fps=info.fps,
+                    size=int(content.get('fileSize', info.size)),
+                    url=url,
+                    mime=info.mime,
+                    original={},
+                )
+                video = Video(
+                    extern_id=guid,
+                    title=title,
+                    description=description,
+                    poster=poster,
+                    duration=float(content.get('duration', info.duration)),
+                    published=published,
+                    tags=keywords,
+                    sources=[source],
+                    original={
+                        'url': url,
+                        'headers': dict(info.headers),
+                        'tag': str(item),
+                    },
+                )
+
+            try:
+                yield video
+
+            except Exception as e:
+                LOGGER.exception(e)
 
 # <channel>
 #     <title>Calm Meditation</title>
 #     <link>http://sample-firetv-web-app.s3-website-us-west-2.amazonaws.com</link>
 #     <language>en-us</language>
 #     <pubDate>Mon, 02 Apr 2018 16:19:56 -0700</pubDate>
 #     <lastBuildDate>Mon, 02 Apr 2018 16:19:56 -0700</lastBuildDate>
 #     <managingEditor>tomjoht@gmail.com (Tom Johnson)</managingEditor>
-#     <description>Contains short videos capturing still scenes from nature with a music background, intended for calming or meditation purposes. When you're stressed out or upset, watch a few videos. As your mind focuses on the small details, let your worries and frustrations float away. The purpose is not to entertain or to distract, but to help calm, soothe, and surface your inner quiet. The videos contain scenes from the San Tomas Aquinas trail in Santa Clara, California.</description>
+#     <description>Contains short videos capturing still scenes from nature
+#       with a music background, intended for calming or meditation purposes.
+#       When you're stressed out or upset, watch a few videos. As your mind
+#       focuses on the small details, let your worries and frustrations float
+#       away. The purpose is not to entertain or to distract, but to help
+#       calm, soothe, and surface your inner quiet. The videos contain scenes
+#       from the San Tomas Aquinas trail in Santa Clara, California.
+#     </description>
 #     <image>
 #         <link>http://sample-firetv-web-app.s3-website-us-west-2.amazonaws.com</link>
 #         <title>Calm Meditation</title>
 #         <url>http://sample-firetv-web-app.s3-website-us-west-2.amazonaws.com/images/calmmeditationlogo_small.png</url>
-#         <description>Contains short videos capturing still scenes from nature with a music background, intended for calming or meditation purposes. When you're stressed out or upset, watch a few videos. As your mind focuses on the small details, let your worries and frustrations float away. The purpose is not to entertain or to distract, but to help calm, soothe, and surface your inner quiet. The videos contain scenes from the San Tomas Aquinas trail in Santa Clara, California.</description>
+#         <description>Contains short videos capturing still scenes from
+#           nature with a music background, intended for calming or
+#           meditation purposes. When you're stressed out or upset, watch a
+#           few videos. As your mind focuses on the small details, let your
+#           worries and frustrations float away. The purpose is not to
+#           entertain or to distract, but to help calm, soothe, and surface
+#           your inner quiet. The videos contain scenes from the San Tomas
+#           Aquinas trail in Santa Clara, California.</description>
 #         <height>114</height>
 #         <width>114</width>
 #     </image>
-    async def crawl(self, url, options=None):
+    async def crawl(self, url, **kwargs):
+        await self.login(url, **kwargs)
+
         async with ScraperSession() as s:
-            r = await s._request(METH_GET, url)
+            r = await s._request(METH_GET, url, proxy=self._proxy)
             try:
-                self.state = {
+                state = {
                     'Last-Modified': parsedate_to_datetime(
                         r.headers['Last-Modified']),
                     'Content-Length': int(r.headers['Content-Length']),
                 }
             except KeyError as e:
                 LOGGER.warning(
-                    'Could not read header: %s, cannot save state', e.args[0])
+                    'Could not read header: %s, unknown state', e.args[0])
+
+            else:
+                if self._state == state:
+                    LOGGER.info('Matching state, no updates')
+                    return
+                else:
+                    self._state = state
+
             soup = BeautifulSoup(await r.text(), 'xml')
             tag = soup.find('channel')
             channel = self.ChannelModel(
                 extern_id=md5sum(url),
                 name=tag.title.text,
                 url=url,
                 description=tag.description.text,
```

## videosrc/crawlers/odysee.py

```diff
@@ -1,70 +1,77 @@
 import re
 import time
-from pprint import pprint
-from urllib.parse import urlparse
-from pprint import pprint
+import logging
+
+from urllib.parse import urlparse, urljoin
 from datetime import datetime
 
 from aiohttp.hdrs import METH_POST
 from aiohttp_scraper import ScraperSession
 
-from videosrc.models import Channel, Video, VideoSource
+from videosrc.crawlers.base import Crawler
+
 
+LOGGER = logging.getLogger(__name__)
+LOGGER.addHandler(logging.NullHandler())
 
 API_URL = 'https://api.na-backend.odysee.com/api/v1/proxy'
+PAGE_SIZE = 20
 
 
-class OdyseeCrawler:
-    def __init__(self, state=None, ChannelModel=Channel, VideoModel=Video,
-                 VideoSourceModel=VideoSource, api_url=API_URL):
-        self.auth = None
-        self.state = state
-        self.ChannelModel = ChannelModel
-        self.VideoModel = VideoModel
-        self.VideoSourceModel = VideoSourceModel
+class OdyseeCrawler(Crawler):
+    def __init__(self, state=0, api_url=API_URL, **kwargs):
+        super().__init__(state=state, **kwargs)
         self.api_url = api_url
-        self.auth_token = None
+        self.auth = None
 
     @staticmethod
     def check_url(url):
         urlp = urlparse(url)
         return urlp.netloc.endswith('odysee.com')
 
     async def _make_request(self, method, params):
         ts = int(time.time())
         async with ScraperSession() as s:
             r = await s.request(
-                    METH_POST,
+                METH_POST,
                 self.api_url,
+                proxy=self._proxy,
                 params={'m': method},
                 json={
                     "jsonrpc": "2.0",
                     "method": method,
                     "params": params,
                     "id": ts,
-                })
+                }
+            )
         return (await r.json())['result']
 
-    async def login(self):
+    async def login(self, url, **kwargs):
+        # url = 'https://api.odysee.com/user/new'
+        url = urljoin(self.api_url, '/user/new')
         async with ScraperSession() as s:
             r = await s._request(
                 METH_POST,
-                'https://api.odysee.com/user/new',
+                url,
+                proxy=self._proxy,
                 data={
                     'auth_token': '',
                     'language': 'en',
-                    'app_id': 'odyseecom692EAWhtoqDuAfQ6KHMXxFxt8tkhmt7sfprEMHWKjy5hf6PwZcHDV542V',
-            })
+                    'app_id': 'odyseecom692EAWhtoqDuAfQ6KHMXxFxt8tkhmt7sfprEMH'
+                              'WKjy5hf6PwZcHDV542V',
+                    }
+            )
         self.auth = (await r.json())['auth_token']
 
     # {
     #     "address": "bMyDG3xgFUK4fao4dU8jq1THhEGhJr5z55",
     #     "amount": "0.002",
-    #     "canonical_url": "lbry://@timcast#c/elon-musk-nuked-ukrainian-gov-from-orbit#5",
+    #     "canonical_url": "lbry://@timcast#c/elon-musk-nuked-ukrainian-gov-fro
+    #                       m-orbit#5",
     #     "claim_id": "5c02f6632888b2462c60fc3e608721b3c45cb402",
     #     "claim_op": "create",
     #     "confirmations": 23,
     #     "height": 1242068,
     #     "is_channel_signature_valid": true,
     #     "meta": {
     #         "activation_height": 1242068,
@@ -76,15 +83,16 @@
     #         "reposted": 0,
     #         "support_amount": "0.0",
     #         "take_over_height": 1242068
     #     },
     #     "name": "elon-musk-nuked-ukrainian-gov-from-orbit",
     #     "normalized_name": "elon-musk-nuked-ukrainian-gov-from-orbit",
     #     "nout": 0,
-    #     "permanent_url": "lbry://elon-musk-nuked-ukrainian-gov-from-orbit#5c02f6632888b2462c60fc3e608721b3c45cb402",
+    #     "permanent_url": "lbry://elon-musk-nuked-ukrainian-gov-from-orbit#5c0
+    #                       2f6632888b2462c60fc3e608721b3c45cb402",
     #     "short_url": "lbry://elon-musk-nuked-ukrainian-gov-from-orbit#5",
     #     "signing_channel": {
     #         "address": "bMyDG3xgFUK4fao4dU8jq1THhEGhJr5z55",
     #         "amount": "0.005",
     #         "canonical_url": "lbry://@timcast#c",
     #         "claim_id": "c9da929d12afe6066acc89eb044b552f0d63782a",
     #         "claim_op": "update",
@@ -102,57 +110,98 @@
     #         "reposted": 1,
     #         "support_amount": "77718.4404",
     #         "take_over_height": 307232
     #         },
     #         "name": "@timcast",
     #         "normalized_name": "@timcast",
     #         "nout": 0,
-    #         "permanent_url": "lbry://@timcast#c9da929d12afe6066acc89eb044b552f0d63782a",
+    #         "permanent_url": "lbry://@timcast#c9da929d12afe6066acc89eb044b552
+    #                           f0d63782a",
     #         "short_url": "lbry://@timcast#c",
     #         "timestamp": 1589902980,
-    #         "txid": "94f2374d50b5b645a05b2bc6fc405d7cbd9f940509d4695f81b68bc18866ceeb",
+    #         "txid": "94f2374d50b5b645a05b2bc6fc405d7cbd9f940509d4695f81b68bc1
+    #                  8866ceeb",
     #         "type": "claim",
     #         "value": {
     #         "cover": {
-    #             "url": "https://thumbnails.lbry.com/banner-UCG749Dj4V2fKa143f8sE60Q"
+    #             "url": "https://thumbnails.lbry.com/banner-UCG749Dj4V2fKa143f
+    #                     8sE60Q"
     #         },
-    #         "description": "Tim Pool brings you breaking news from around the world and commentary on top news topics in Politics and Cultural issues\naround the world.\n\nStay tuned for live news, livestreams, breaking stories, everyday and a new podcast episode of \"The Culture War\" every Sunday at 4pm.\n\nUse the email below for any business inquiries.",
+    #         "description": "Tim Pool brings you breaking news from around the
+    #                         world and commentary on top news topics in
+    #                         Politics and Cultural issues\naround the world.
+    #                         \n\nStay tuned for live news, livestreams,
+    #                         breaking stories, everyday and a new podcast
+    #                         episode of \"The Culture War\" every Sunday at
+    #                         4pm.\n\nUse the email below for any business
+    #                         inquiries.",
     #         "locations": [
     #             {
     #             "country": "US"
     #             }
     #         ],
-    #         "public_key": "029da7c3648c9b9f9e46e6c15eaa91a1a502baa389bc8fea07c0fe35be99c5bc47",
+    #         "public_key": "029da7c3648c9b9f9e46e6c15eaa91a1a502baa389bc8fea07
+    #                        c0fe35be99c5bc47",
     #         "public_key_id": "bPCWHNqyteXatNEHJR73UeZdpC13CsJMRs",
     #         "tags": [
     #             "news",
     #             "technology"
     #         ],
     #         "thumbnail": {
     #             "url": "https://thumbnails.lbry.com/UCG749Dj4V2fKa143f8sE60Q"
     #         },
     #         "title": "Tim Pool"
     #         },
     #         "value_type": "channel"
     #     },
     #     "timestamp": 1665779521,
-    #     "txid": "782e0c0566843c399e95684eea4532b26cc4bc9441e4a3ae1dd9cedb895be214",
+    #     "txid": "782e0c0566843c399e95684eea4532b26cc4bc9441e4a3ae1dd9cedb895b
+    #              e214",
     #     "type": "claim",
     #     "value": {
-    #         "description": "Elon Musk NUKED Ukrainian Gov FROM ORBIT SHUTTERING Starlink After They Told Him To F OFF. After Elon Musk proposed peace Ukrainian Government officials insulted him, now the tech billionaire is saying he won't pay for Starlink anymore in Ukraine.\n\nAlready reports have surfaced that elon shuttered the use of starlink in Crimea and one person accused Elon of having spoken to Putin about it. Elon Musk denies this however.\n\nBut one thing is clear, Ukraine should be grateful that Elon Musk was assisting in the ukraine war effort and if they want his support they should apologize.\n\nAt any rate it seems that Biden and the Democrats do not have a plan for how to win and the end result will simply be world war three\n\n#starlink \n#elonmusk \n#ukraine \n\nBecome A Member And Protect Our Work at http://www.timcast.com\n\nMy Second Channel - https://www.youtube.com/timcastnews\nPodcast Channel - https://www.youtube.com/TimcastIRL\n\nMerch - http://teespring.com/timcast\n\nMake sure to subscribe for more travel, news, opinion, and documentary with Tim Pool everyday.\n...\nhttps://www.youtube.com/watch?v=gxOz3q0jrEY",
+    #         "description": "Elon Musk NUKED Ukrainian Gov FROM ORBIT
+    #                         SHUTTERING Starlink After They Told Him To F OFF.
+    #                         After Elon Musk proposed peace Ukrainian
+    #                         Government officials insulted him, now the tech
+    #                         billionaire is saying he won't pay for Starlink
+    #                         anymore in Ukraine.\n\nAlready reports have
+    #                         surfaced that elon shuttered the use of starlink
+    #                         in Crimea and one person accused Elon of having
+    #                         spoken to Putin about it. Elon Musk denies this
+    #                         however.\n\nBut one thing is clear, Ukraine
+    #                         should be grateful that Elon Musk was assisting
+    #                         in the ukraine war effort and if they want his
+    #                         support they should apologize.\n\nAt any rate it
+    #                         seems that Biden and the Democrats do not have a
+    #                         plan for how to win and the end result will
+    #                         simply be world war three\n\n#starlink
+    #                         \n#elonmusk \n#ukraine \n\nBecome A Member And
+    #                         Protect Our Work at http://www.timcast.com\n\nMy
+    #                         Second Channel -
+    #                         https://www.youtube.com/timcastnews\nPodcast
+    #                         Channel -
+    #                         https://www.youtube.com/TimcastIRL
+    #                         \n\nMerch -
+    #                         http://teespring.com/timcast\n\n
+    #                         Make sure to subscribe for more travel, news,
+    #                         opinion, and documentary with Tim Pool everyday.
+    #                         \n...
+    #                         \nhttps://www.youtube.com/watch?v=gxOz3q0jrEY",
     #         "languages": [
     #             "en"
     #         ],
     #         "license": "Copyrighted (contact publisher)",
     #         "release_time": "1665777612",
     #         "source": {
-    #             "hash": "f360f8a33fd12fb633a21bc67f56aef2a68f9423c1c2c33557a53321dc2921c0ba7df2df5de3fb40f98c24a88db4c2c8",
+    #             "hash": "f360f8a33fd12fb633a21bc67f56aef2a68f9423c1c2c33557a5
+    #                      3321dc2921c0ba7df2df5de3fb40f98c24a88db4c2c8",
     #             "media_type": "video/mp4",
     #             "name": "elon-musk-nuked-ukrainian-gov.mp4",
-    #             "sd_hash": "bf39ccb675f0d041b938c7d31e04de45224ecd0a9d74241fc39c035e763e5d92d64feb8d3e56b2b592667a6d4236f570",
+    #             "sd_hash": "bf39ccb675f0d041b938c7d31e04de45224ecd0a9d74241fc
+    #                         39c035e763e5d92d64feb8d3e56b2b592667a6d4236f570",
     #             "size": "194873679"
     #         },
     #         "stream_type": "video",
     #         "tags": [
     #             "news",
     #             "technology",
     #             "biden",
@@ -166,49 +215,55 @@
     #             "ukraine starlink",
     #             "ukraine war",
     #             "world war three"
     #         ],
     #         "thumbnail": {
     #             "url": "https://thumbnails.lbry.com/gxOz3q0jrEY"
     #         },
-    #         "title": "Elon Musk NUKED Ukrainian Gov FROM ORBIT SHUTTERING Starlink After They Told Him To F OFF",
+    #         "title": "Elon Musk NUKED Ukrainian Gov FROM ORBIT SHUTTERING
+    #                   Starlink After They Told Him To F OFF",
     #         "video": {
     #             "duration": 1948,
     #             "height": 1080,
     #             "width": 1920
     #         }
     #     },
     #     "value_type": "stream"
     # },
-    async def _iter_videos(self, channel_id, state):
+    async def _iter_videos(self, channel_id):
         page_number = 1
-        release_time = f"<{self.state}"
-        if state:
-            release_time = [release_time, f'>{state}']
+        release_time = f'>{self._state}'
         while True:
             result = await self._make_request('claim_search', {
-                "page_size": 2,
+                "page_size": PAGE_SIZE,
                 "page": page_number,
-                "claim_type": ["stream","repost"],
+                "claim_type": ["stream", "repost"],
                 "no_totals": True,
-                "not_tags": ["porn","porno","nsfw","mature","xxx","sex","creampie","blowjob","handjob","vagina","boobs","big boobs","big dick","pussy","cumshot","anal","hard fucking","ass","fuck","hentai"],
+                "not_tags": [
+                    "porn", "porno", "nsfw", "mature", "xxx", "sex",
+                    "creampie", "blowjob", "handjob", "vagina", "boobs",
+                    "big boobs", "big dick", "pussy", "cumshot", "anal",
+                    "hard fucking", "ass", "fuck", "hentai",
+                ],
                 "order_by": ["release_time"],
                 "has_source": True,
                 "channel_ids": [channel_id],
                 "release_time": release_time,
             })
             items = result['items']
             for item in items:
                 published = datetime.fromtimestamp(
                     int(item['value']['release_time']))
-                stream = await self._make_request('get', { 'uri': item['short_url'] })
+                stream = await self._make_request(
+                    'get', {'uri': item['short_url']})
                 source = self.VideoSourceModel(
                     extern_id=item['claim_id'],
                     url=stream['streaming_url'],
                     size=item['value']['source']['size'],
+                    mime=item['value']['source']['media_type'],
                     width=item['value']['video']['width'],
                     height=item['value']['video']['height'],
                     original=item,
                 )
                 video = self.VideoModel(
                     extern_id=item['claim_id'],
                     title=item['value']['title'],
@@ -216,25 +271,35 @@
                     poster=item['value']['thumbnail']['url'],
                     duration=item['value']['video']['duration'],
                     tags=item['value']['tags'],
                     published=published,
                     sources=[source],
                     original=item,
                 )
-                yield video, self.state
-            if len(items) + 1 < int(result['page_size']):
+
+                try:
+                    yield video
+
+                except Exception as e:
+                    LOGGER.exception(e)
+
+                else:
+                    self._state = published
+
+            if len(items) + 1 < PAGE_SIZE:
                 break
-            break
+
             page_number += 1
 
-    async def crawl(self, url, options=None):
+    async def crawl(self, url, **kwargs):
+        await self.login(url, **kwargs)
+
         # https://odysee.com/@timcast:c
-        state, self.state = self.state, int(time.time())
         urlp = urlparse(url)
-        channel_name = re.match('/@(\w+):c', urlp.path).group(1)
+        channel_name = re.match(r'/@(\w+):c', urlp.path).group(1)
         lbry_url = f'lbry://@{channel_name}#c'
         result = await self._make_request('resolve', {
             'urls': [
                 lbry_url,
             ],
             'include_purchase_receipt': True,
             'include_is_my_output': True,
@@ -246,8 +311,8 @@
             name=channel_name,
             url=url,
             title=value['title'],
             description=value['description'],
             poster=value['cover']['url'],
         )
         channel_id = result['claim_id']
-        return channel, self._iter_videos(channel_id, state)
+        return channel, self.iter_videos(channel_id, **kwargs)
```

## videosrc/crawlers/peertube.py

```diff
@@ -1,98 +1,103 @@
-import numbers
 import logging
 from urllib.parse import urlparse, urljoin, urlunparse
 from os.path import split as pathsplit
 from datetime import datetime
-from pprint import pprint
 
-from aiohttp.hdrs import METH_GET, METH_POST
+from aiohttp.hdrs import METH_POST
 from aiohttp_scraper import ScraperSession
 
-from videosrc.models import Channel, Video, VideoSource
-from videosrc.utils import md5sum
+from videosrc.utils import md5sum, url2mime
+from videosrc.crawlers.base import Crawler
 
 
 LOGGER = logging.getLogger(__name__)
 LOGGER.addHandler(logging.NullHandler())
 
 # '2022-07-31T01:16:56.624Z'
-DATETIME_FMT = '%Y-%m-%dT%H:%M:%S%z'
+# '2022-07-31T00:54:44.991Z'
+DATETIME_FMT = '%Y-%m-%dT%H:%M:%S.%fZ'
 
 
-def maybe_parse_date(date_str):
-    if date_str is None:
+def maybe_parse_date(s):
+    if s is None:
         return None
 
     try:
-        return datetime.strptime(date_str, DATETIME_FMT)
+        return datetime.strptime(s, DATETIME_FMT)
 
     except ValueError:
         LOGGER.exception('Error parsing datetime')
         return datetime.utcnow()
 
 
 def parse_channel_name(url):
     # http://cesium.tv/video-channels/btimby_channel
     # http://cesium.tv/c/btimby_channel@cesium.tv:80/videos
     urlp = urlparse(url)
     if urlp.path.startswith('/c/') and '@' in urlp.path:
         return urlp, pathsplit(urlp.path)[1]
     if urlp.path.startswith('/video-channels/'):
         port = urlp.port or 80 if urlp.scheme == 'http' else 443
-        channel_name = f'{pathsplit(url.path)[1]}@{urlp.netloc}:80'
+        channel_name = f'{pathsplit(url.path)[1]}@{urlp.netloc}:{port}'
         return urlp, channel_name
 
 
-class PeerTubeCrawler:
-    def __init__(self, state=None, ChannelModel=Channel, VideoModel=Video,
-                 VideoSourceModel=VideoSource):
+class PeerTubeCrawler(Crawler):
+    auth_fields = {
+        'username': 'String',
+        'password': 'String',
+    }
+
+    def __init__(self, state=0, **kwargs):
+        super().__init__(state=state, **kwargs)
         self.auth = {}
-        self.state = state or 0
-        self.ChannelModel = ChannelModel
-        self.VideoModel = VideoModel
-        self.VideoSourceModel = VideoSourceModel
 
     @staticmethod
     def check_url(url):
         urlp = urlparse(url)
         if urlp.path.startswith('/c/') and '@' in urlp.path:
             return True
         if urlp.path.startswith('/video-channels/'):
             return True
 
     async def _iter_videos(self, url):
         url += '/videos'
+        params = {
+            'count': 25,
+            'sort': '-publishedAt',
+            'skipCount': 'true',
+        }
+        if self._state:
+            params['start'] = self._state
         async with ScraperSession() as s:
-            results = await s.get_json(url, params={
-                'start': self.state,
-                'count': 25,
-                'sort': '-publishedAt',
-                'skipCount': 'true',
-            }, **self.auth)
+            results = await s.get_json(
+                url, params=params, proxy=self._proxy, **self.auth
+            )
 
         urlp = urlparse(url)
         for result in results['data']:
             url = urlunparse((
                 urlp.scheme,
                 urlp.netloc,
                 f"/api/v1/videos/{result['shortUUID']}", '', '', ''))
             async with ScraperSession() as s:
-                obj = await s.get_json(url, **self.auth)
+                obj = await s.get_json(url, proxy=self._proxy, **self.auth)
             files = obj.pop('files')
 
             sources = []
             for file in files:
                 sources.append(self.VideoSourceModel(
                     extern_id=md5sum(file['fileUrl']),
-                    width = file['resolution']['id'],
-                    height = None,
-                    fps = file['fps'],
-                    size = file['size'],
-                    url = file['fileUrl'],
+                    width=file['resolution']['id'],
+                    height=None,
+                    fps=file['fps'],
+                    size=file['size'],
+                    mime=url2mime(file['fileUrl']),
+                    url=file['fileUrl'],
                     original=file,
                 ))
 
             tags = list(obj['tags'])
             tags.append(obj['category']['label'])
 
             video = self.VideoModel(
@@ -102,52 +107,73 @@
                 duration=obj['duration'],
                 original=obj,
                 published=maybe_parse_date(obj['publishedAt']),
                 tags=tags,
                 sources=sources,
             )
 
-            self.state += 1
-            yield video, self.state
+            try:
+                yield video
+
+            except Exception as e:
+                LOGGER.exception(e)
+
+            self._state += 1
+
+    async def login(self, url, **kwargs):
+        try:
+            username = kwargs['username']
+            password = kwargs['password']
+        except KeyError:
+            LOGGER.warning('No credentials, skipping login')
+            return
 
-    async def login(self, url, username, password):
         async with ScraperSession() as s:
-            r = await s.get_json(urljoin(url, '/api/v1/oauth-clients/local/'))
+            r = await s.get_json(
+                urljoin(url, '/api/v1/oauth-clients/local/'),
+                proxy=self._proxy)
             params = r.json()
-            r = await s._request(METH_POST, urljoin(url, '/api/v1/users/token/'), data={
-                'client_id': params['client_id'],
-                'client_secret': params['client_secret'],
-                'grant_type': 'password',
-                'username': username,
-                'password': password,
-            })
+            r = await s._request(
+                METH_POST,
+                urljoin(url, '/api/v1/users/token/'),
+                proxy=self._proxy,
+                data={
+                    'client_id': params['client_id'],
+                    'client_secret': params['client_secret'],
+                    'grant_type': 'password',
+                    'username': username,
+                    'password': password,
+                },
+            )
             token = await r.json()
         self.auth = {
             'headers': {
                 'Authorization': f"Bearer {token['access_token']}"
             }
         }
 
-    async def crawl(self, url, options=None):
+    async def crawl(self, url, **kwargs):
+        await self.login(url, **kwargs)
+
         urlp, channel_name = parse_channel_name(url)
 
         # NOTE: We are assuming the channel name is local to the instance
         # we are connected to. If this is not a safe assumption, then we
         # need to store the full channel name including host info.
         # http://cesium.tv/api/v1/video-channels/btimby_channel@cesium.tv:80/videos?start=0&count=0&sort=-publishedAt
         # http://cesium.tv/api/v1/video-channels/btimby_channel@cesium.tv:80
         url = urlunparse((
             urlp.scheme,
             urlp.netloc,
             f'api/v1/video-channels/{channel_name}', '', '', ''))
 
         async with ScraperSession() as s:
-            results = await s.get_json(url, **self.auth)
+            results = await s.get_json(url, proxy=self._proxy, **self.auth)
 
         channel = self.ChannelModel(
             extern_id=md5sum(channel_name),
             name=channel_name,
             title=results['displayName'],
             url=results['url'],
         )
 
-        return channel, self._iter_videos(url)
+        return channel, self.iter_videos(url, **kwargs)
```

## videosrc/crawlers/rumble.py

```diff
@@ -1,110 +1,114 @@
 import re
 import json
+import logging
 
 from os.path import split as pathsplit
 from urllib.parse import urlparse, urljoin
 from datetime import datetime
-from pprint import pprint
 
 from aiohttp_scraper import ScraperSession
 from bs4 import BeautifulSoup
 
-from videosrc.models import Channel, Video, VideoSource
-from videosrc.utils import md5sum
+from videosrc.utils import md5sum, url2mime
+from videosrc.crawlers.base import Crawler
 
 
+LOGGER = logging.getLogger(__name__)
+LOGGER.addHandler(logging.NullHandler())
+
 # Used to parse JSON out of a block of javascript.
 JSON_EXTRACT = re.compile(r'\w\.\w\["\w{6,7}"\]=({.*?});')
 HTML_ERASE = re.compile(r'<.*>')  # Removes HTML / CSS.
 FUNC_ERASE = re.compile(r',loaded:\w\(\)')  # Removes an function reference.
 
 
-async def get_embed_details(embed_url):
+async def get_embed_details(embed_url, proxy=None):
     async with ScraperSession() as s:
-        embed_page = await s.get_html(embed_url)
+        embed_page = await s.get_html(embed_url, proxy=proxy)
         m = JSON_EXTRACT.search(embed_page)
         if not m:
             raise Exception('Could not extract data from javascript')
         data = m.group(1)
         data = HTML_ERASE.sub('', data)
         data = FUNC_ERASE.sub('', data)
         return json.loads(data)
 
 
-async def get_video_details(url):
+async def get_video_details(url, proxy=None):
     async with ScraperSession() as s:
-        video_page = BeautifulSoup(await s.get_html(url), 'html.parser')
+        video_page = BeautifulSoup(
+            await s.get_html(url, proxy=proxy), 'html.parser')
         script_tag = video_page.find('script', type='application/ld+json')
         video_details = json.loads(script_tag.text)
         embed_url = video_details[0]['embedUrl']
-        return await get_embed_details(embed_url)
+        return await get_embed_details(embed_url, proxy=proxy)
 
 
 def parse_date(s):
     # 2022-10-18T13:02:12+00:00
     return datetime.strptime(s, '%Y-%m-%dT%H:%M:%S%z')
 
 
-class RumbleCrawler:
-    def __init__(self, state=None, ChannelModel=Channel,
-                 VideoModel=Video, VideoSourceModel=VideoSource):
-        self.state = state
-        self.ChannelModel = ChannelModel
-        self.VideoModel = VideoModel
-        self.VideoSourceModel = VideoSourceModel
-
+class RumbleCrawler(Crawler):
     @staticmethod
     def check_url(url):
         urlp = urlparse(url)
         return urlp.netloc.endswith('rumble.com')
 
-    async def login(self):
-        pass
-
     async def _iter_videos(self, url, page):
         for li in page.find_all('li', class_='video-listing-entry'):
             url = urljoin(url, li.article.a['href'])
-            video_details = await get_video_details(url)
+            video_details = await get_video_details(url, proxy=self._proxy)
             published = parse_date(video_details['pubDate'])
-            if self.state and self.state > published:
+            if self._state and self._state > published:
                 LOGGER.info('Video published before given state')
                 break
             sources = [
                 self.VideoSourceModel(
                     extern_id=md5sum(src['url']),
                     width=src['meta']['w'],
                     height=src['meta']['h'],
                     size=src['meta']['size'],
+                    mime=url2mime(src['url']),
                     url=src['url'],
                     original=src,
                 ) for src in video_details['ua']['mp4'].values()
             ]
             video = self.VideoModel(
                 extern_id=md5sum(url),
                 title=li.article.h3.text,
                 poster=li.article.img['src'],
                 duration=video_details['duration'],
                 published=published,
                 sources=sources,
                 original=str(li),
             )
-            yield video, published
 
-    async def crawl(self, url):
+            try:
+                yield video
+
+            except Exception as e:
+                LOGGER.exception(e)
+
+            else:
+                self._state = published
+
+    async def crawl(self, url, **kwargs):
         # https://rumble.com/user/vivafrei
         urlp = urlparse(url)
         pparts = pathsplit(urlp.path.strip('/'))
 
         async with ScraperSession() as s:
-            page = BeautifulSoup(await s.get_html(url), 'html.parser')
+            html = await s.get_html(url, proxy=self._proxy)
+            page = BeautifulSoup(html, 'html.parser')
 
         thumb = page.find('img', class_='listing-header--thumb')
         poster = thumb.src if thumb else None
         channel = self.ChannelModel(
             extern_id=md5sum(url),
             name=pparts[-1],
             url=url,
             poster=poster,
         )
 
-        return channel, self._iter_videos(url, page)
+        return channel, self.iter_videos(url, page, **kwargs)
```

## videosrc/crawlers/timcast.py

```diff
@@ -1,26 +1,22 @@
 import re
 import time
-import json
 import asyncio
 import logging
 
 from urllib.parse import urljoin, urlparse
-from datetime import datetime
-from pprint import pprint
 
 import pyppeteer
 from pyppeteer.errors import PyppeteerError
-from aiohttp.hdrs import METH_HEAD
 from aiohttp_scraper import ScraperSession
 from bs4 import BeautifulSoup
 
-from videosrc.models import Channel, Video, VideoSource
-from videosrc.utils import get_tag_text, md5sum
+from videosrc.utils import get_tag_text, md5sum, url2mime
 from videosrc.crawlers.rumble import get_embed_details, parse_date
+from videosrc.crawlers.base import Crawler
 
 
 LOGGER = logging.getLogger(__name__)
 LOGGER.addHandler(logging.NullHandler())
 
 RUMBLE_DOMAIN = re.compile(r'^https://rumble.com/embed/')
 JSON_EXTRACT = re.compile(r'g\.f\["\w{6,7}"\]=({.*}),loaded:d\(\)')
@@ -43,46 +39,51 @@
 def _no_images(request):
     if request.resourceType() == 'image':
         request.abort_()
     else:
         request.continue_()
 
 
-class TimcastCrawler:
-    def __init__(self,  state=None, ChannelModel=Channel,
-                 VideoModel=Video, VideoSourceModel=VideoSource):
+class TimcastCrawler(Crawler):
+    auth_fields = {
+        'username': 'String',
+        'password': 'String',
+    }
+
+    def __init__(self,  **kwargs):
+        super().__init__(**kwargs)
         self.auth = {}
-        self.state = state
-        self.ChannelModel = ChannelModel
-        self.VideoModel = VideoModel
-        self.VideoSourceModel = VideoSourceModel
 
     @staticmethod
     def check_url(url):
         urlp = urlparse(url)
         return urlp.netloc.endswith('timcast.com')
 
-    async def _login(self, url, username, password, headless=True, timeout=2000):
+    async def _login(self, url, username, password, headless=True,
+                     timeout=2000):
+        args = [
+            '--start-maximized',
+            '--no-sandbox',
+            '--disable-setuid-sandbox',
+        ]
+        if self._proxy:
+            args.append(f'--proxy-server={self._proxy}')
         browser = await pyppeteer.launch(
             headless=headless,
-            args=[
-                '--start-maximized',
-                '--no-sandbox',
-                '--disable-setuid-sandbox'
-            ],
+            args=args,
             ignoreHTTPSError=True,
             handleSIGINT=False,
             handleSIGTERM=False,
             handleSIGHUP=False
         )
 
         page = await browser.newPage()
 
         try:
-            await page.setViewport({ 'width': 1366, 'height': 768 })
+            await page.setViewport({'width': 1366, 'height': 768})
 
             LOGGER.debug('Opening url: %s', url)
             await page.goto(url, timeout=timeout, waitFor='networkidle2')
 
             LOGGER.debug('Typing')
             await page.type(U_FIELD, username)
             await page.type(P_FIELD, password)
@@ -95,95 +96,118 @@
                 cookieStr.append(f"{cookie['name']}={cookie['value']}")
             return {'headers': {'Cookies': '; '.join(cookieStr)}}
 
         finally:
             await page.close()
             await browser.close()
 
-    async def login(self, url, username, password, headless=True, retry=3, timeout=2000):
+    async def login(self, url, **kwargs):
+        try:
+            username = kwargs['username']
+            password = kwargs['password']
+        except KeyError:
+            LOGGER.warning('No credentials, skipping login')
+            return
+
+        retry = kwargs.pop('retry', 3)
         url = urljoin(url, '/login/')
 
         for i in range(1, 1 + retry):
             try:
                 LOGGER.debug('Login attempt %i', i)
                 self.auth = await self._login(
-                    url, username, password, headless=headless, timeout=timeout
-                )
+                    url, username, password, **kwargs)
                 break
 
             except (PyppeteerError, asyncio.TimeoutError):
                 if i == retry:
                     raise
 
                 LOGGER.exception('Login failed, retrying')
                 time.sleep(3 ** i)
 
     async def _iter_videos(self, url, page):
-        grid = page.find('div', class_='t-grid:s:fit:2 t-grid:m:fit:4 t-pad:25pc:top')
+        grid = page.find(
+            'div', class_='t-grid:s:fit:2 t-grid:m:fit:4 t-pad:25pc:top')
         for article in grid.find_all('div', class_='article'):
-            date = article.find('div', class_='summary').div.text
-            (m, d, y) = map(int, date.split('.'))
-            state = datetime(y + 2000, m, d)
-            if self.state and self.state > state:
-                LOGGER.info('Video published before given state')
-                break
-
             video_link = article.find('a', class_='image')
             thumbnail = video_link.img['src']
             description = video_link.img['alt']
             video_page_url = video_link['href']
             video_page_url = urljoin(url, urlparse(video_page_url).path)
             async with ScraperSession() as s:
                 video_page = BeautifulSoup(
-                    await s.get_html(video_page_url, **self.auth),
+                    await s.get_html(video_page_url, proxy=self._proxy,
+                                     **self.auth),
                     'html.parser')
             iframe_tag = video_page.find('iframe')
             embed_url = iframe_tag['src']
-            video_details = await get_embed_details(embed_url)
+            video_details = await get_embed_details(
+                embed_url, proxy=self._proxy)
+            pubDate = parse_date(video_details['pubDate'])
+
+            if self._state and pubDate < self._state:
+                LOGGER.info('Video published before last state %s', pubDate)
+                return
+
             sources = [
                 self.VideoSourceModel(
                     extern_id=md5sum(src['url']),
                     width=src['meta']['w'],
                     height=src['meta']['h'],
                     size=src['meta']['size'],
+                    mime=url2mime(src['url']),
                     url=src['url'],
                     original=src,
                 ) for src in video_details['ua']['mp4'].values()
             ]
-            yield self.VideoModel(
+            video = self.VideoModel(
                 extern_id=md5sum(video_page_url),
                 title=description,
                 poster=thumbnail,
                 duration=video_details['duration'],
-                published=parse_date(video_details['pubDate']),
+                published=pubDate,
                 original=str(article),
                 sources=sources,
-            ), state
+            )
+
+            try:
+                yield video
+
+            except Exception as e:
+                LOGGER.exception(e)
+
+            else:
+                self._state = pubDate
 
     async def _iter_pages(self, url, page):
         page_num = 1
         async with ScraperSession() as s:
             while True:
                 LOGGER.debug('Scraping page %i', page_num)
                 async for video in self._iter_videos(url, page):
                     yield video
                 page_num += 1
                 page_url = url + f'page/{page_num}/'
                 urlp = urlparse(page_url)
                 if not page.find('a', href=urlp.path[:-1]):
                     LOGGER.debug('No more pages')
                     break
-                page = BeautifulSoup(
-                    await s.get_html(page_url, **self.auth), 'html.parser')
+                html = await s.get_html(
+                    page_url, proxy=self._proxy, **self.auth)
+                page = BeautifulSoup(html, 'html.parser')
+
+    async def crawl(self, url, **kwargs):
+        await self.login(url, **kwargs)
 
-    async def crawl(self, url, options=None):
         async with ScraperSession() as s:
             page = BeautifulSoup(
-                await s.get_html(url, **self.auth), 'html.parser')
+                await s.get_html(
+                    url, proxy=self._proxy, **self.auth), 'html.parser')
             title = get_tag_text(page, 'h1')
             channel = self.ChannelModel(
                 extern_id=md5sum(url),
                 url=url,
                 name=title,
             )
 
-        return channel, self._iter_pages(url, page)
+        return channel, self._iter_pages(url, page, **kwargs)
```

## videosrc/models/__init__.py

```diff
@@ -16,14 +16,15 @@
 class VideoSource:
     extern_id: str
     original: dict
     width: int
     height: int
     size: int
     url: str
+    mime: str
     fps: int = None
 
 
 @dataclass
 class Video:
     extern_id: str
     original: dict
```

## Comparing `videosrc-0.1.6.dist-info/LICENSE` & `videosrc-0.1.7.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `videosrc-0.1.6.dist-info/RECORD` & `videosrc-0.1.7.dist-info/RECORD`

 * *Files 19% similar despite different names*

```diff
@@ -1,23 +1,27 @@
-tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tests/test_html.py,sha256=gBXv0Q7SbP9WmQBc5ZmOTzCM3b4ybrfZhJC6R9YRzHg,2815
-tests/test_mrss.py,sha256=Tw3NtYhxDlBSG_cH8A27WGJYmr5iOaYPJSRDb0zSkYk,6428
-tests/test_odysee.py,sha256=oQDzq3U0yBthrnMFKm0_b3WnjNxumfxw4YXRwGKXclM,34148
-tests/test_peertube.py,sha256=CQuwQUCSNDp-DO-9IdyGe5foXkxNgHmu0X0tt94JHk0,29432
-tests/test_rumble.py,sha256=QpVb4zt0z4K3swnARRb0ccEgLbnJUi5E6AP7naowLBU,76420
-tests/test_timcast.py,sha256=E64Gg_R-d7pP1LEz2si2nKERd78R8OF6RsIuPCvVY-o,191649
-videosrc/__init__.py,sha256=HIzC_c1QqLFKkzOnpvPy40cCP7VN-7MhcsnluVPHrnY,943
-videosrc/__main__.py,sha256=bjGhzwJmCPEczOqjGEGSF0zNZ_wAeOlItVpC0NnnpA4,859
-videosrc/utils.py,sha256=zZaipk2Ig0qDtajL-EmKflL0MkigdqcgEbmu5HPMq_k,4041
-videosrc/crawlers/__init__.py,sha256=42_ZrSAwOeDS1l1MFXz92moTjSN1iHsjyD0beKFhNZo,429
-videosrc/crawlers/html.py,sha256=Wtl7p9vRfqQw6g4bYqfTRIUhPRDJpWYd6ovdEL3atfg,3058
-videosrc/crawlers/mrss.py,sha256=CzF8aXSGbEBLQbo_FB-d6AuQtUqLfZwk31JjjPXIl60,6504
-videosrc/crawlers/odysee.py,sha256=dV9ydSwGxIbT-wEQFrhXavCghlpgoCiYlgpTUFbSQms,11273
-videosrc/crawlers/peertube.py,sha256=mOnX2llAPwTLzxpAXDLIvg6vfk97_8pcPV-LM-Rs6iM,5182
-videosrc/crawlers/rumble.py,sha256=bNo3XEJWNVe5YOlOP5-ojMvW-SoXrzNQ-ibjpKv2eNo,3701
-videosrc/crawlers/timcast.py,sha256=x7bhQjRfQrRkwWijEZgLyvU42qJs9riCvj3DeffVHgU,6403
-videosrc/models/__init__.py,sha256=VFeUcXvOrB6uYDOot0dDikDHRg8NYsLzUB2CLWT3okY,612
-videosrc-0.1.6.dist-info/LICENSE,sha256=ZrmE4LhjnartZCuclnrGM1kWT3L1WxtXC84vnHMOxTc,1066
-videosrc-0.1.6.dist-info/METADATA,sha256=FIOn-le-EKGITtTbzzVzRgNicSp3M04QgDEb5BJZ4Gg,1435
-videosrc-0.1.6.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-videosrc-0.1.6.dist-info/top_level.txt,sha256=GVj7Kn2EZJ5LjJjdb6uJtVlPu5yaZ8MjWnmK76wRyAk,15
-videosrc-0.1.6.dist-info/RECORD,,
+tests/__init__.py,sha256=8SKf9fWRSWybH4iBXfXnBL5plPieOHtOGQU_YoYlLlk,311
+tests/test_html.py,sha256=Lr_IPVetCznEEU_pIF6KkOy5rS6bFV3aBk8CNcB29BU,2862
+tests/test_mrss.py,sha256=IL1VFvg9dUBiaXW7fPHrWgrUxMBkD5HtH9yNgY-6yl4,6475
+tests/test_odysee.py,sha256=yI7yiUwM5YQH_LuvtG27vhl3HSDQDfUSPdOhdIn7WX8,34252
+tests/test_peertube.py,sha256=AwZnrBoY7ZYlWw92h_4A6vrUmE9i66d998mSnhZ9MT8,29900
+tests/test_rumble.py,sha256=dsY5hrNijIZ5JAP6VBaEdhslahqyDvvz5UAHrP2S2AA,76475
+tests/test_timcast.py,sha256=a_yzZcI_P-PlhQNzvc2EUX2mmsMvJrW7C0T267mOVQI,191646
+tests/test_twitter.py,sha256=NyxGIzYw6ge6AdNuQZbCibZec1F10xqKlDeXyhXTW7w,1112
+videosrc/__init__.py,sha256=kw3tNEbF5DeKIxwdHlNjo5RyyL-6SliFEiIWZiqBKr4,1685
+videosrc/__main__.py,sha256=6SpxV8YXkwM5Tmq4YCJxqQF5aXTHCR0oTY9ISFiKCVg,1812
+videosrc/errors.py,sha256=4TuFGWdjaBKSzzUglz05x4VLHos9AjTm6_IodWrm_2M,189
+videosrc/utils.py,sha256=mQos14ZTZuXioRc1GtLcnMFAWX1u82qxgkGr9M7Q5zk,5296
+videosrc/crawlers/__init__.py,sha256=tEwn_Q7xIYuXhUP9t-CFqM8WPwEP_2n6v8s9FuaHwE4,500
+videosrc/crawlers/base.py,sha256=OIoZfmIkF8qVOYkdBBh1gRRCsMHLpVP_6EiDb-dG638,2256
+videosrc/crawlers/html.py,sha256=-AOjNEfGI4THK4OMfVwlG3K17HVa1ctMZoM3sBHOFwI,3565
+videosrc/crawlers/mrss.py,sha256=cwmzIoJApoAhkSMNgYO4OFYS-b5-wwRfFtFzLGWB6dY,6869
+videosrc/crawlers/odysee.py,sha256=fR5UebO9H8hodm0hUmSunLraxbVSxJ_SSjSGUQ-oUTs,12880
+videosrc/crawlers/peertube.py,sha256=ndNjyZBGkgN6_w1PYdcAWqb54Yu5umllLQS4coCp3S8,5709
+videosrc/crawlers/rumble.py,sha256=_sYRANtT-vd3aoKx4krmLxlYE08WLqVb3KbtWTUcxPQ,3804
+videosrc/crawlers/timcast.py,sha256=JF4EKj3F4gfkAy-WpyopUQgpeSNXtzFlRFM3rky9ADQ,6783
+videosrc/crawlers/twitter.py,sha256=MGcv_D6EOJKqBz2VU-CQOn0ZnPZPhD1swVYZ-hIEkG4,3121
+videosrc/models/__init__.py,sha256=33PwZuf3LbSS1FZSVq-RVn2PbejFASLIItv005ixce4,626
+videosrc-0.1.7.dist-info/LICENSE,sha256=ZrmE4LhjnartZCuclnrGM1kWT3L1WxtXC84vnHMOxTc,1066
+videosrc-0.1.7.dist-info/METADATA,sha256=vxpCvj6mI7kUfjSZEVkXEyML13iMKEmSTiA5E0vhQMs,2011
+videosrc-0.1.7.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+videosrc-0.1.7.dist-info/top_level.txt,sha256=GVj7Kn2EZJ5LjJjdb6uJtVlPu5yaZ8MjWnmK76wRyAk,15
+videosrc-0.1.7.dist-info/RECORD,,
```

